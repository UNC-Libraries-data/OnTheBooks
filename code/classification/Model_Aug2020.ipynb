{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook documents the model fit during the first phase of *On the Books: Jim Crow and Algorithms of Resistance*, as of August 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../training_set/training_set_v0_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We performed simple preprocessing on the text:\n",
    "* Replaced hyphenated and line broken words with unbroken words.\n",
    "* Removed section numbering from the law text (\"section_text\").\n",
    "* Removed all non-ASCII characters (most of these were OCR errors).\n",
    "* Converted all words to lower case.\n",
    "* Removed stopwords based on `nltk`'s default list.\n",
    " * We also removed any words occuring in less than 2 or more than 1000 documents.\n",
    "* We used session or volume identified (\"csv\") information to extract a numeric year.  In the case of multi-year volumes (e.g. 1956-1957) the earlier year was used.\n",
    "\n",
    "Then we convert the text into a document-term matrix, augmented with year and law type variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "repl = lambda m: m.group(\"letter\")\n",
    "\n",
    "#Fix hyphenated words\n",
    "train_df[\"text\"] = train_df.text.str.replace(r\"-[ \\|]+(?P<letter>[a-zA-Z])\",repl).astype(\"str\")\n",
    "train_df[\"section_text\"] = train_df.section_text.str.replace(r\"-[ \\|]+(?P<letter>[a-zA-Z])\",repl).astype(\"str\")\n",
    "train_df[\"section_text\"] = [re.sub(r'- *\\n+(\\w+ *)', r'\\1\\n',r) for r in train_df[\"section_text\"]]\n",
    "\n",
    "#Remove section titles (e.g. \"Sec. 1\") from law text.\n",
    "train_df[\"start\"] = train_df.section_raw.str.len().fillna(0).astype(\"int\")\n",
    "train_df[\"section_text\"] = train_df.apply(lambda x: x['section_text'][(x[\"start\"]):], axis=1).str.strip()\n",
    "\n",
    "#Remove all non-ASCII characters\n",
    "train_df[\"section_text\"] = train_df[\"section_text\"].str.replace(r\"[^\\x00-\\x7F]\", \"\", regex=True)\n",
    "\n",
    "law_list = [word_tokenize(r.lower()) for r in train_df.section_text]\n",
    "stop_words = stopwords.words('english')\n",
    "law_list = [[word for word in law if word not in stop_words] for law in law_list]\n",
    "\n",
    "#Extract a numeric year variable\n",
    "train_df[\"year\"] = train_df.sess.str.slice(start = 0, stop = 4).astype(\"float\")\n",
    "train_df.loc[train_df.sess.isna(),\"year\"] = train_df.csv.str.extract(\"(\\d{4})\")\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "#Remove terms appearing in less than 2 or more than 1000 documents, then convert to document-term matrix.\n",
    "vect = CountVectorizer(tokenizer=dummy,preprocessor=dummy, decode_error = \"ignore\",\n",
    "                      min_df = 2, max_df = 1000)\n",
    "dtm = vect.fit_transform(law_list)\n",
    "\n",
    "#Add year and law type variables.\n",
    "extra_df = train_df.loc[:,[\"year\",\"type\"]].copy()\n",
    "extra_df = pd.get_dummies(extra_df, columns = [\"type\"], prefix = [\"type\"])\n",
    "X = scipy.sparse.hstack((dtm,extra_df.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Details\n",
    "\n",
    "The `fit_params` below were fit using a 80-20 training-test split, followed by 10-fold cross validation on the training set.  We will include a basic template of our model selection process later this year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_params =  {'colsample_bytree': 0.3, 'gamma': 0.3, 'learning_rate': 0.3, \n",
    "               'max_depth': 20, 'min_child_weight': 1, 'n_estimators': 50, \n",
    "               'scale_pos_weight': 5}\n",
    "all_mod = XGBClassifier(**fit_params)\n",
    "all_modfit = all_mod.fit(X, train_df.assessment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XGBoost classifier outperformed the other models selected.  Read more about XGBoost [here](https://arxiv.org/abs/1603.02754).  \n",
    "\n",
    "After fitting, we used probability calibration to adjust the model probabilities to better reflect the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_mod = CalibratedClassifierCV(all_modfit, cv=10, method=\"isotonic\")\n",
    "calibrated_modfit = calibrated_mod.fit(X, train_df.assessment)\n",
    "\n",
    "\n",
    "train_df[\"base_labels\"] = all_modfit.predict(X)\n",
    "train_df[\"base_probs\"] = all_modfit.predict_proba(X)[:,1]\n",
    "train_df[\"calibrated_probs\"] = calibrated_modfit.predict_proba(X)[:,1]\n",
    "train_df[\"calibrated_labels\"] = (train_df.calibrated_probs > 0.9).astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reported any laws with a calibrated probability over 90% as as Jim Crow laws with a source of \"model\", unless they were also later confirmed by an expert, in which case they were labeled as \"model and expert\".  We chose to be conservative at this point to minimize false positives and since this project will continue over the coming year, allowing us more time to fine tune the modeling process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
