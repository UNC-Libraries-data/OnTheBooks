{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Structured Data from OCR'ed Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<< Previous module: [Structuring OCR'ed Text Data](05-StructuringOCRData.ipynb) <<**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*1.5-2.5 hours*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <strong>Learning Objectives:</strong>\n",
    "    <p>By the end of this module, you should be able to</p>\n",
    "    <ul>\n",
    "        <li>develop potential research questions based on a text dataset;</li>\n",
    "        <li>perform basic exploratory analysis using Python, Voyant, Pandas, and other tools.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Reading a Dataset](#reading-dataset)\n",
    "- [Exploratory Questions](#exploratory-questions)\n",
    "    - [Quantitive](#quantitative)\n",
    "    - [Topics & Themes](#topics)\n",
    "    - [Time](#time)\n",
    "    - [Space](#space)\n",
    "- [Wrap Up & Next Steps](#next-steps)\n",
    "- [Resources](#resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <p><strong><em>Congratulations!</em></strong> You've made it to the final module. All of the previous tutorials were dedicated to the processes that transform a text corpus into data: Optical Character Recognition, error correction, and data structuring. In this tutorial, we'll introduce ways that this data can now be explored as a means for developing research questions and new project directions.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a long tutorial. If you need to, you can complete each of the major sections (Quantitative, Topics & Themes, Time, and Space) separately. \n",
    "\n",
    "**Note that changes may not be saved in Binder. If you want to save your work:** You can always save your work by following our instructions to install and run these modules locally, or you can use `File > Download as...` in the menu above. When you return, reload Binder, and click the Jupyter icon at the top of this notebook. Click `Upload` at the top of the file list to upload your saved version of this tutorial.\n",
    "\n",
    "**Each time you return, please run the first script below to reload the dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a Dataset <a class=\"anchor\" id=\"reading-dataset\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading in the dataset that we created in the [last module](05-StructuringOCRData.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pandas library. We'll continue to use it to manipulate \n",
    "# our dataset.\n",
    "import pandas\n",
    "\n",
    "# Read in the jclaws_dataset.csv that we created in the last module. \n",
    "# Store the dataset in a pandas dataframe called \"df\".\n",
    "df = pandas.read_csv(\"jclaws_dataset.csv\", sep =\"|\")\n",
    "\n",
    "# Show us a preview of the dataset.\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take a moment to examine this preview of the Jim Crow laws dataset.** What kinds of questions might we be able to ask and answer using computational methods? What kinds of questions might we *not* be able to ask and answer using computational methods? *Before moving on to the next section, write down a few questions that come to mind.*\n",
    "\n",
    "### Hints for Reading a Dataset\n",
    "- **Look for patterns & outliers:** As you study a dataset, look for common values or what could look like trends in information categories, quantitities, locations, temporalities, etc. Equally, look for exceptions, or values that are unique or occur rarely.\n",
    "- **Read the columns:** Instead of reading one row at a time, try reading a column at a time. What kinds of information are available in each of the columns? Are they numeric, temporal, categorial (e.g. text), or other kinds of data? Can any of the rows be *compared* in any way?\n",
    "- **Consider quantities:** What kinds of information can be *counted* and *compared*? How might quantitative data be useful or not?\n",
    "- **Consider topics:** For columns with textual data, what can we learn about common or uncommon themes, topics, or categories within the dataset?\n",
    "- **Consider time:** Is it possible to view the dataset over time?\n",
    "- **Consider space:** Can any of the information in the dataset be mapped to explore possible spatial patterns?\n",
    "\n",
    "If elements such as space or time are not visible in the current dataset, what kinds of changes might we need to make to the dataset to make those accessible here? **This dataset's shape (its current columns, for example) is not set in stone.** We can and will continue to modify the dataset's structure by changing how data are represented and/or adding new columns throughout this module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Questions <a class=\"anchor\" id=\"exploratory-questions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jim Crow laws dataset contains a variety of information types: quantitative (e.g. WordCount, IdentifiedBy, and LawType), thematic or topical (SectionText and ChapterTitle), temporal (Volume), and spatial (SectionText and ChapterTitle). Here are some of the questions that they bring to mind:\n",
    "\n",
    "**Quantitative:** \n",
    "- How many laws are there in this corpus?\n",
    "- How many laws were identified by experts? How many were identified by the computer model?\n",
    "- How many laws are categorized as Public Laws, Private Laws, or Session Laws?\n",
    "- How many laws per volume have been identified?\n",
    "- What is the average length of a Jim Crow law? \n",
    "\n",
    "**Topics & Themes:**\n",
    "- Which words or phrases occur most frequently in the corpus?\n",
    "- How are commonly appearing words used in context?\n",
    "\n",
    "**Temporal:**\n",
    "- How many laws per year have been identified?\n",
    "\n",
    "**Spatial:**\n",
    "- How many laws were passed at a local level? At a state level?\n",
    "- Where in North Carolina were Jim Crow laws adopted?\n",
    "\n",
    "This list of questions is *not exhaustive*, and we will not cover all of the questions here but demonstrate the tools you can use to try to answer them. Each question category, or information type, is also *not exclusive*. Often, questions in one category above draw on information found in another category. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative <a class=\"anchor\" id=\"quantitative\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing quantitative information in our dataset can give us an overview, or big picture, of our dataset that can help us to identify additional questions or avenues for exploration. This information *likely will not* provide any definitive or detailed understanding of our corpus but can provide some direction as we dive deeper.\n",
    "\n",
    "- How many laws are there in this corpus?\n",
    "- How many laws were identified by experts? How many were identified by the computer model?\n",
    "- How many laws are categorized as Public Laws, Private Laws, or Session Laws?\n",
    "- How many laws per volume have been identified?\n",
    "- What is the average length of a Jim Crow law? \n",
    "- Do longer or shorter laws focus on only certain topics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many laws are there in this corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first question may seem elementary, but we need to know the size of our dataset in order to understand the relative significance of other quantitative questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the dataframe (df), count the number of rows (.count()) based \n",
    "# on the first (lefthand) column, \"[0]\", which is the dataframe index. \n",
    "# The index applies a unique value to each row, beginning with 0.\n",
    "df.count()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above script should print a number, which is the number of rows in the corpus. In other words, the number of Jim Crow laws in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many laws were identified by experts? How many were identified by the computer model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer this question, we need to ask Python to count all of the *unique* values in the IdentifiedBy column. First, let's ask for a list of unique values so that we know how many to expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the dataframe (df), look in the \"IdentifiedBy\" column,\n",
    "# and print a list of all the unique values in that column.\n",
    "df.IdentifiedBy.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should have been returned a list containing the following values:\n",
    "\n",
    "- **model:** The law was identified by the computer model as being Jim Crow based on the training set of expert-identified Jim Crow laws.\n",
    "- **xpert:** The law was identified by at least one expert as being Jim Crow.\n",
    "- **nan:** The IdentifiedBy field for this row is empty or contains unrecognizable data. (\"nan,\" or \"NaN,\" stands for \"Not a Number.\")\n",
    "\n",
    "Now that we know which values to expect, let's count how many times they appear in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the dataframe (df) column \"IdentifiedBy,\"\n",
    "# count all of the unique values.\n",
    "df['IdentifiedBy'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command should have produced a list of the values in \"IdentifiedBy\" and the number of time they appear. Note that \"nan\" is not counted, meaning that there are no \"nan\" values in the column.\n",
    "\n",
    "While a list such as this can be fairly straightforward to read and consider, we may want to view such results visually to help us quickly grasp this quantitative information. A simple way to do this is to use the <a href=\"https://matplotlib.org/\" target=\"blank\">Matplotlib</a> Python library alongside Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we'll need to create a new dataframe from the IdentifiedBy \n",
    "# counts above:\n",
    "\n",
    "# Store the list of values created above in a variable called \n",
    "# \"valueCounts\".\n",
    "valueCounts = df['IdentifiedBy'].value_counts()\n",
    "\n",
    "# Create a new dataframe, \"identifiedByDF,\" using valueCounts.\n",
    "identifiedByDF = pandas.DataFrame(valueCounts)\n",
    "\n",
    "# Set the row numbers (index) for the new dataframe.\n",
    "identifiedByDF = identifiedByDF.reset_index()\n",
    "\n",
    "# Create the column names for the IdentifiedBy dataframe.\n",
    "identifiedByDF.columns = ['IdentifiedBy', 'Count']\n",
    "\n",
    "# Show a preview of the IdentifiedBy dataframe.\n",
    "identifiedByDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have our new dataframe, we can create a bar chart:\n",
    "\n",
    "# We just need one part of matplotlib, the pyplot module.\n",
    "# We'll import pyplot and call it \"plt\" for short.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assign the x and y axes to specific columns.\n",
    "identifiedByDF.plot(x ='IdentifiedBy', y='Count', kind = 'bar')\n",
    "\n",
    "# Show us the resulting bar chart.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's consider the data:** Which method most frequently identified a law as Jim Crow? What might this say about the corpus and the process used to identify Jim Crow laws?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many laws are categorized as Public Laws, Private Laws, or Session Laws?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply a similar approach as we did above to investigate the different law types. Can you modify each code block below to produce an answer to this question? (Look for hints in the # comments!)\n",
    "\n",
    "1. Identify the unique values in the column specifying law types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"COLUMNNAME\" with the correct column name for law types.\n",
    "df.COLUMNNAME.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Count the unique vales in the \"LawType\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"COLUMNNAME\" with the correct column name for law types.\n",
    "df['COLUMNNAME'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a new dataframe to hold the unique values and their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"COLUMNNAME\" with the correct column name for law types.\n",
    "valueCounts = df['COLUMNNAME'].value_counts()\n",
    "\n",
    "# Replace \"cHANGEMEDF\" with a new name for the law types dataframe.\n",
    "# Make sure you change ALL mentions of this dataframe below.\n",
    "cHANGEMEDF = pandas.DataFrame(valueCounts)\n",
    "\n",
    "# Set the row numbers (index) for the new dataframe.\n",
    "cHANGEMEDF = cHANGEMEDF.reset_index()\n",
    "\n",
    "# Create the column names for the law types dataframe.\n",
    "# Replace \"COLUMN 1\" and \"COLUMN 2\" with descriptive names!\n",
    "cHANGEMEDF.columns = ['COLUMN 1', 'COLUMN 2']\n",
    "\n",
    "# Show the IdentifiedBy dataframe.\n",
    "cHANGEMEDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Optionally, create a bar chart to aid quick reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just in case, we'll reimport the pyplot module from matplotlib.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Replace \"cHANGEMEDF\" with your name for the law types dataframe.\n",
    "# Replace \"COLUMN 1\" and \"COLUMN 2\" with your column names!\n",
    "cHANGEMEDF.plot(x ='COLUMN 1', y='COLUMN 2', kind = 'bar')\n",
    "\n",
    "# Show us the resulting bar chart.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What kinds of new questions or observations does this process reveal about law types in the corpus? What would you do next to continue exploring different law types?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many laws per volume have been identified? <a class=\"anchor\" id=\"laws-per-volume\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with calculating the quantities of different law types and identifiers, we can use Pandas' `value_counts()` function to count how many times a volume appears in the dataset. To do this, which column should we count? Insert the column name you think should be used in the script below and then run the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"COLUMNNAME\" with the correct column name for volume years.\n",
    "df['COLUMNNAME'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at this list as a line chart. Note that we've combined the dataframe creation and chart creation steps from the previous question into one script. *Does this change how the steps are performed? If so, how?* \n",
    "\n",
    "Previously, we've created bar charts, but we've altered the script slightly to create a line chart. *Can you identify where we've made that change?*\n",
    "\n",
    "Again, add the correct column name to the script below before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just in case, we'll reimport the pyplot module from matplotlib.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Replace \"COLUMNNAME\" with the correct column name for volume years.\n",
    "valueCounts = df['COLUMNNAME'].value_counts()\n",
    "\n",
    "# Create a new dataframe for volumes.\n",
    "volumesDF = pandas.DataFrame(valueCounts)\n",
    "\n",
    "# Set the row numbers (index) for the new dataframe.\n",
    "volumesDF = volumesDF.reset_index()\n",
    "\n",
    "# Create the column names for the volumes dataframe.\n",
    "# Replace \"COLUMN 1\" and \"COLUMN 2\" with descriptive names!\n",
    "# HINT: COLUMN 1 will be volume years, and COLUMN 2 will hold \n",
    "# the number of times that volume year appears.\n",
    "volumesDF.columns = ['COLUMN 1', 'COLUMN 2']\n",
    "\n",
    "# Plot the line graph, assigning each column to an axis.\n",
    "# Replace \"COLUMN 1\" and \"COLUMN 2\" with your column names!\n",
    "volumesDF.plot(x ='COLUMN 1', y='COLUMN 2', kind = 'line')\n",
    "\n",
    "# Show us the resulting line chart.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, this graph seems to tell us a compelling story, but take a closer look: **What's wrong with this graph?** HINT: look at the list of unique values that we calculated a moment ago. \n",
    "\n",
    "The chart's data are currently sorted by count -- that is, the highest number of laws per volume down to the lowest. That's not technically *wrong*, but it might be showing us a false narrative if what we are expecting to see is the number of laws per volume *by order of publication*. Let's now rerun the above script but add in Pandas' `sort_values()` function to order the chart by year. \n",
    "\n",
    "*This time, the column names are provided:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just in case, we'll reimport the pyplot module from matplotlib.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count the number of times each volume year appears in the dataframe.\n",
    "valueCounts = df['Volume'].value_counts()\n",
    "\n",
    "# Create a new dataframe for volumes.\n",
    "volumesDF = pandas.DataFrame(valueCounts)\n",
    "\n",
    "# Set the row numbers (index) for the new dataframe.\n",
    "volumesDF = volumesDF.reset_index()\n",
    "\n",
    "# Create the column names for the volumes dataframe.\n",
    "volumesDF.columns = ['Volume', 'Count']\n",
    "\n",
    "# We ADDED this line to sort the volumes dataframe by the year/volume.\n",
    "volumesDF = volumesDF.sort_values(by='Volume')\n",
    "\n",
    "# Plot the line graph, assigning each column to an axis.\n",
    "volumesDF.plot(x ='Volume', y='Count', kind = 'line')\n",
    "\n",
    "# Show us the resulting line chart.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're getting somewhere. You should now see the Volumes ordered from earliest to latest along the X axis. But what's *still* not quite right about this chart? We used year(s) of publication to identify each volume, so this graph also appears to show the number of Jim Crow laws passed per year, but is it *actually*? Are *all years* between 1865-1966 represented? How does the line above reveal or obscure this information? Is there a better way to visualize laws per volume?\n",
    "\n",
    "Skip ahead to the Time section where we ask [\"How many laws per year have been identified?\"](#laws-per-year) to see how we can visualize all years 1865-1966."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the average length of a Jim Crow law?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we have included word count in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can add together all of the word counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty variable to collect the sum of all word counts.\n",
    "total = 0\n",
    "\n",
    "# For each word count in the WordCount column:\n",
    "for count in df['WordCount']:\n",
    "    \n",
    "    # Add the word count to the total.\n",
    "    total = count + total\n",
    "\n",
    "# Show us the total number of words in the Jim Crow corpus.\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's divide the total word count by the total number of laws to get an average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the total number of words in the corpus by the number of laws.\n",
    "# Replace \"NUMBER OF LAWS\" with the number generated earlier in \n",
    "# this module. (\"How many laws are there in this corpus?\")\n",
    "averageCount = total / NUMBER OF LAWS\n",
    "\n",
    "print(averageCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What might average word count tell us? What does it **not** tell us? What kinds of new questions does it raise?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics & Themes <a class=\"anchor\" id=\"topics\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computational text analysis, or text mining, is one method for exploring text corpuses. There are many ways, with varying complexity, to perform text analysis. Common text analysis methods used to explore a corpus include\n",
    "\n",
    "- <strong><mark style=\"background-color:lightgreen\">word frequency</mark></strong>: Counting the number of times a word appears in a corpus. A common goal with this method is to identify popular or frequently appearing words and possible patterns in their usage.\n",
    "- **collocation**: Counting occurrences of 2 or more words appearing near one another. Collocation can be used to identify possible relationships between the appearance of different words close together in a corpus.\n",
    "- <strong><mark style=\"background-color:lightgreen\">**n-grams**</mark></strong>: Counting the appearance of 2+ word phrases. As with word frequency, a common goal is to identify frequently occurring phrases and possible patterns in their usage.\n",
    "- <strong><mark style=\"background-color:lightgreen\">concordance</mark></strong>: Viewing the context(s) in which a word appears.\n",
    "- **entity recognition**: Identifying names, places, etc.\n",
    "- **dictionary tagging**: Looking for specific words within a text.\n",
    "- **topic modeling**: Grouping texts in a corpus together based on the presence of specific terms. The aim with topic modeling is to create semantically meaningful groups and to look for patterns in those groupings.\n",
    "\n",
    "We will look at only some of these methods (<mark style=\"background-color:lightgreen\">highlighted in green</mark>) using a few different tools, but know that there are many resources available for learning text analysis. Check [Resources](#resources) below for suggested tutorials to get you started.\n",
    "\n",
    "As a reminder, here are the questions we'll be addressing:\n",
    "\n",
    "- Which words or phrases occur most frequently in the corpus?\n",
    "- How are commonly appearing words used in context?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the corpus first with Voyant. <a class=\"anchor\" id=\"voyant\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we look at these specific questions, let's take a few minutes to explore the corpus in <a href=\"https://voyant-tools.org/\" target=\"blank\">Voyant</a>, a digital humanities tool for exploring digitized texts:\n",
    "\n",
    "<a href=\"https://voyant-tools.org/\" target=\"blank\"><img src=\"images/10-explore-02.jpeg\" width=\"90%\" style=\"margin:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Screen capture of Voyant's landing page.\" title=\"Screen capture of Voyant's landing page.\"/></a>\n",
    "\n",
    "1. [Download the Jim Crow corpus text file](https://cdr.lib.unc.edu/downloads/6q182r84s?locale=en) and save it to an easy-to-find location on your computer (e.g. the Desktop).\n",
    "2. Navigate to <a href=\"https://voyant-tools.org/\" target=\"blank\">Voyant</a>.\n",
    "3. Click \"upload,\" and open the Jim Crow corpus text file in Voyant.\n",
    "4. A new page will load that looks something like this:\n",
    "\n",
    "<img src=\"images/10-explore-03.jpeg\" width=\"90%\" style=\"margin:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Screen capture showing the Jim Crow laws corpus in Voyant's default interface.\" title=\"Screen capture showing the Jim Crow laws corpus in Voyant's default interface.\"/>\n",
    "\n",
    "5. Take a look at each of the 5 smaller windows in Voyant. What do they show? How can you interact with them? When you click on a word or icon in one window, what happens in other windows? \n",
    "\n",
    "6. Hover over or click on a blue question mark (<span style=\"font-weight:600; color:blue\">?</span>) to learn more about each window.\n",
    "\n",
    "<img src=\"images/10-explore-04.jpeg\" width=\"50%\" style=\"margin:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Screen capture showing Voyant's explanation of the Cirrus visualization.\" title=\"Screen capture showing Voyant's explanation of the Cirrus visualization.\"/>\n",
    "\n",
    "7. We can use the word cloud, \"Cirrus,\" window in the top left to begin exploring word frequency. The larger words in this visualization appear more frequently across the corpus. Which words do you see? \n",
    "\n",
    "<img src=\"images/10-explore-05.jpeg\" width=\"50%\" style=\"margin:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Screen capture showing Voyant's Cirrus visualization.\" title=\"Screen capture showing Voyant's Cirrus visualization.\"/>\n",
    "\n",
    "8. Use the \"terms\" slider below the word cloud to increase the number of words shown in the word cloud.\n",
    "\n",
    "9. Click on \"Terms\" next to \"Cirrus\" above the word cloud to see a list of words in order by frequency. \n",
    "\n",
    "<img src=\"images/10-explore-06.jpeg\" width=\"50%\" style=\"margin:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Screen capture showing Voyant's Terms visualization.\" title=\"Screen capture showing Voyant's Terms visualization.\"/>\n",
    "\n",
    "10. Clicking on a word can show us when it is used across the text in the \"Trends\" window at top right. The corpus is separated as evenly as possible into segments along the X axis.\n",
    "\n",
    "<img src=\"images/10-explore-07.jpeg\" width=\"50%\" style=\"margin:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Screen capture showing Voyant's Trends visualization.\" title=\"Screen capture showing Voyant's Trends visualization.\"/>\n",
    "\n",
    "11. We can also see its usage across the corpus in the \"Contexts\" <a href=\"https://en.wikipedia.org/wiki/Key_Word_in_Context\" target=\"blank\">keyword-in-context concordance</a> window at bottom right. Click the + icon to the left of one of the entries to see a larger view of one usage. Use the \"expand\" slider to increase or decrease the amount of text shown here. \n",
    "\n",
    "<img src=\"images/10-explore-08.jpeg\" width=\"50%\" style=\"margin:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Screen capture showing Voyant's Terms visualization.\" title=\"Screen capture showing Voyant's Terms visualization.\"/>\n",
    "\n",
    "12. The \"Reader\" in the top middle can show further context for individual instances. Click on a word in \"reader\" to change the \"Trends\" and \"Contexts\" windows.\n",
    "\n",
    "<img src=\"images/10-explore-09.jpeg\" width=\"50%\" style=\"margin:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Screen capture showing Voyant's Reader window with a term selected.\" title=\"Screen capture showing Voyant's Reader window with a term selected.\"/>\n",
    "\n",
    "13. Let's return to the Cirrus word cloud visualization. There are likely several words that occur very frequently that provide less insight than their smaller counterparts. Examples might include the verbs \"shall\" and \"said\" or the abbreviation \"sec\" for \"section.\" We might also want to filter out \"north,\" \"carolina,\" and \"state.\" Many of the very common words, known as <a href=\"https://en.wikipedia.org/wiki/Stop_word\" target=\"blank\">stopwords</a> in computing, have already been filtered out. We can view and add to this list by clicking the toggle icon next to the blue question mark (<span style=\"font-weight:600; color:blue\">?</span>) at the top right of the Cirrus window:\n",
    "\n",
    "<img src=\"images/10-explore-10.jpeg\" width=\"50%\" style=\"margin:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Screen capture showing the options toggle icon.\" title=\"Screen capture showing the options toggle icon.\"/>\n",
    "\n",
    "14. An Options popup window will appear, displaying a number of different settings. For this module, we'll focus only on the stopwords. However, <a href=\"https://guides.library.ucsc.edu/DS/Resources/Voyant#s-lg-box-wrapper-29088760\" traget=\"blank\">this tutorial</a> explains more about working with Cirrus in Voyant. Next to \"Stopwords Auto-detect,\" click \"Edit List.\"\n",
    "\n",
    "<img src=\"images/10-explore-11.jpeg\" width=\"50%\" style=\"margin:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Screen capture showing the options popup window.\" title=\"Screen capture showing the options popup window.\"/>\n",
    "\n",
    "15. A list of numbers, punctuation marks, and words will appear. Scroll through these to get a sense of what has been excluded from the word cloud. We can remove or add words to this list. Try adding words such as \"north,\" \"carolina,\" \"state,\" \"laws,\" \"shall,\" \"said,\" and any other commonly used words in this corpus that you might wish to exclude from analysis. Click \"Save.\"\n",
    "\n",
    "<img src=\"images/10-explore-12.jpeg\" width=\"50%\" style=\"margin:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Screen capture showing the stop words list.\" title=\"Screen capture showing the stop words list.\"/>\n",
    "\n",
    "16. Your new stopword list has been saved on Voyant's server and should now by named \"keywords-XXXX\" with some string of letters and numbers. Clicking \"Reset\" in the Options popup window will delete your customized list, and clicking \"Confirm\" will save it. You can return later and edit this list. For now, click \"Confirm.\"\n",
    "\n",
    "<img src=\"images/10-explore-13.jpeg\" width=\"50%\" style=\"margin:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Screen capture showing the edited options popup window.\" title=\"Screen capture showing the edited options popup window.\"/>\n",
    "\n",
    "17. View the new word cloud: how has the visualization changed? What does the Terms list show to be the most common words? What kinds of questions come to mind? Are there terms listed that surprise you? Why or why not? Make a short list of these and use Cirrus and the other Voyant visualizations to explore them. \n",
    "\n",
    "<img src=\"images/10-explore-14.jpeg\" width=\"50%\" style=\"margin:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Screen capture showing the revised Cirrus word cloud.\" title=\"Screen capture showing the revised Cirrus word cloud.\"/>\n",
    "\n",
    "You can take as much or as little time exploring Voyant as you like--you could even try answering the remaining questions in this section using Voyant. Save the URL for the Voyant page you've created, and you'll be able to come back to it later.\n",
    "\n",
    "You can learn more about Voyant by reading its <a href=\"https://voyant-tools.org/docs/#!/guide/about\" target=\"blank\">tutorials and documentation</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which words occur most frequently in the corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We began by exploring the corpus of North Carolina Jim Crow laws using Voyant, and we saw one way, using the Terms list, to view the corpus' most frequently appearing words. Remember that the structure of this list changes depending on whether we add or remove stopwords. Now, let's look at how we can answer this question using Python.\n",
    "\n",
    "In the [How to OCR module](05-StructuringOCRData.ipynb), we used the <a href=\"https://www.nltk.org/index.html\" target=\"blank\">NLTK (Natural Language Processing Toolkit)</a> to count the number of total words in each law. This time, we'll use NLTK to count the number of times each word appears across the corpus. Here is a simple way to do this.\n",
    "\n",
    "First, let's make sure we can access the corpus text file *in the same folder as this module.* This script may look familiar because we used it in the [previous module](05-StructuringOCRData.ipynb) as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requests helps us call up a webpage, or link to a file stored online,\n",
    "# and access the content.\n",
    "import requests\n",
    "\n",
    "# Create a variable to hold the direct link to the text file.\n",
    "url = 'https://cdr.lib.unc.edu/downloads/6q182r84s?locale=en'\n",
    "\n",
    "# Here's where we use the requests module to call up\n",
    "# the content at the url we specified above.\n",
    "r = requests.get(url)\n",
    "\n",
    "# Create and open a new empty text file.\n",
    "with open('on_the_books_text_jc_all.txt', 'wb') as f:\n",
    "    \n",
    "    # Write the contents of the online file into the new file.\n",
    "    f.write(r.content)\n",
    "\n",
    "# When finished, print the following:\n",
    "print('Corpus downloaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the word_tokenize and punkt modules from the nltk \n",
    "# (\"Natural Language Processing Kit\") library.\n",
    "# NLTK is a powerful toolset we can use to manipulate \n",
    "# and analyze text data.\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import punkt\n",
    "\n",
    "# We'll also use Pandas to create a dataframe (table) of word \n",
    "# frequencies.\n",
    "import pandas as pd\n",
    "\n",
    "# Open the corpus file, and read it.\n",
    "corpus = open(\"on_the_books_text_jc_all.txt\", \"r\")\n",
    "corpus = corpus.read()\n",
    "\n",
    "# Create a list of words.\n",
    "tokens = word_tokenize(corpus)\n",
    "\n",
    "# Make all the words lowercase so that words such as \"THE\", \n",
    "# \"The\", and \"the\" are all counted as the same word.\n",
    "tokens = [token.lower() for token in tokens]\n",
    "\n",
    "# Count the number of times each word appears in the corpus.\n",
    "wordFreq = nltk.FreqDist(tokens)\n",
    "\n",
    "# Make a new Pandas dataframe for easier reading.\n",
    "wordFreqDF = pd.DataFrame(list(wordFreq.items()),columns = ['word','count']) \n",
    "\n",
    "# Sort the dataframe so that the most frequently occurring terms \n",
    "# appear first.\n",
    "wordFreqDF.sort_values(by=['count'], inplace=True, ascending=False)\n",
    "\n",
    "# Show us the first 20 entries in the dataframe.\n",
    "# We ask for 21 rows because the row with column names is included.\n",
    "wordFreqDF.head(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What do you notice about the most common terms in this list? How are these similar to or different from the word cloud we created in Voyant?*\n",
    "\n",
    "The list currently begins with a number of stopwords and punctuation. We can remove these as we did in Voyant, this time using NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the word_tokenize module from the nltk \n",
    "# (\"Natural Language Processing Kit\") library.\n",
    "# NLTK is a powerful toolset we can use to manipulate\n",
    "# and analyze text data.\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Download the stopwords list from NLTK.\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# We'll also use Pandas to create a dataframe (table) of \n",
    "# word frequencies.\n",
    "import pandas as pd\n",
    "\n",
    "# Open the corpus file and read it.\n",
    "corpus = open(\"on_the_books_text_jc_all.txt\", \"r\")\n",
    "corpus = corpus.read()\n",
    "\n",
    "# Select the English list of stopwords from NLTK.\n",
    "stopWords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Create a list of words.\n",
    "tokens = word_tokenize(corpus)\n",
    "\n",
    "# Make all the words lowercase so that words such as \n",
    "# \"THE\", \"The\", and \"the\" are all counted as the same word.\n",
    "tokens = [token.lower() for token in tokens]\n",
    "\n",
    "# Create a new empty list for the tokens NOT considered stopwords.\n",
    "filteredTokens = [] \n",
    "\n",
    "# Go through the tokens list, pull out each word that is not \n",
    "# a stopword, and add it to the filteredTokens list.\n",
    "for w in tokens: \n",
    "    if w not in stopWords: \n",
    "        filteredTokens.append(w) \n",
    "\n",
    "# Count the number of times each word appears in the corpus.\n",
    "wordFreq = nltk.FreqDist(filteredTokens)\n",
    "\n",
    "# Make a new Pandas dataframe for easier reading.\n",
    "wordFreqDF = pd.DataFrame(list(wordFreq.items()),columns = ['word','count']) \n",
    "\n",
    "# Sort the dataframe so that the most frequently occurring \n",
    "# terms appear first.\n",
    "wordFreqDF.sort_values(by=['count'], inplace=True, ascending=False)\n",
    "\n",
    "# Show us the first 20 entries in the dataframe.\n",
    "# We ask for 21 rows because the row with column names is included.\n",
    "wordFreqDF.head(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How has the list changed this time? Are there more words or punctuation we might want to remove?* \n",
    "\n",
    "If so, run the script one more time with an additional line where we can add words and punctuation to our stopwords list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the word_tokenize module from the nltk \n",
    "# (\"Natural Language Processing Kit\") library.\n",
    "# NLTK is a powerful toolset we can use to manipulate \n",
    "# and analyze text data.\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Download the stopwords list from NLTK.\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# We'll also use Pandas to create a dataframe (table) of \n",
    "# word frequencies.\n",
    "import pandas as pd\n",
    "\n",
    "# Open the corpus file, and read it.\n",
    "corpus = open(\"on_the_books_text_jc_all.txt\", \"r\")\n",
    "corpus = corpus.read()\n",
    "\n",
    "# Select the English list of stopwords from NLTK.\n",
    "stopWords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Create a list of words to add to NLTK's stopwords list. \n",
    "# ADD TO THIS LIST! Make sure they are lowercase, surrounded\n",
    "# by quote marks (' '), and separated from other words by\n",
    "# a comma.\n",
    "addedWords = [',','.','shall','said',':',';','may','sec','north','carolina','state','(',')','act','ch','chapter','one','law','laws']\n",
    "\n",
    "# Add the new words to NLTKs existing stopwords list.\n",
    "stopWords.extend(addedWords)\n",
    "\n",
    "# Create a list of words.\n",
    "tokens = word_tokenize(corpus)\n",
    "\n",
    "# Make all the words lowercase so that words such as \n",
    "# \"THE\", \"The\", and \"the\" are all counted as the same word.\n",
    "tokens = [token.lower() for token in tokens]\n",
    "\n",
    "# Create a new empty list for the tokens NOT considered stopwords.\n",
    "filteredTokens = [] \n",
    "\n",
    "# Go through the tokens list, pull out each word that is not \n",
    "# a stopword, and add it to the filteredTokens list.\n",
    "for w in tokens: \n",
    "    if w not in stopWords: \n",
    "        filteredTokens.append(w) \n",
    "\n",
    "# Count the number of times each word appears in the corpus.\n",
    "wordFreq = nltk.FreqDist(filteredTokens)\n",
    "\n",
    "# Make a new Pandas dataframe for easier reading.\n",
    "wordFreqDF = pd.DataFrame(list(wordFreq.items()),columns = ['word','count']) \n",
    "\n",
    "# Sort the dataframe so that the most frequently occurring terms \n",
    "# appear first. Change ascending to True to see the least common \n",
    "# terms first.\n",
    "wordFreqDF.sort_values(by=['count'], inplace=True, ascending=False)\n",
    "\n",
    "# Show us the first 20 entries in the dataframe.\n",
    "# We ask for 21 rows because the row with column names is included.\n",
    "wordFreqDF.head(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How has the list changed now? If you're still seeing common words that you'd like to exclude, add them to the \"addWords\" list in the script and rerun it.*\n",
    "\n",
    "If you'd like to see the **least common words**, return to the second to last line of code in the script above, and change `ascending=False` to `ascending=True`. Then rerun the script:\n",
    "\n",
    "`# Sort the dataframe so that the most frequently occurring terms appear first.\n",
    "wordFreqDF.sort_values(by=['count'], inplace=True, ascending=True)`\n",
    "\n",
    "*What do you notice about the least common words?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which phrases occur most frequently in the corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen how we can look for frequent words, let's look for frequent phrases. In computing, the term `n-gram` is used to refer to phrases -- co-occurring words or parts of a sentence. The `n` in `n-gram` refers to the fact that the phrase (gram) could have any number of units. Technically, we just finished finding common *unigrams* -- one-word units. Now, we'll look for common two- and three-word units (bigrams and trigrams, respectively).\n",
    "\n",
    "We'll start by looking for **bigrams, or two-word phrases.** The following scripts may at first look very similar to the word frequency script above. *Can you spot the differences? What are these new lines of code doing?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the word_tokenize module from the nltk \n",
    "# (\"Natural Language Processing Kit\") library.\n",
    "# NLTK is a powerful toolset we can use to manipulate \n",
    "# and analyze text data. Note the new NLTK modules we \n",
    "# are importing here:\n",
    "# sent_tokenize will help us split laws into lists of\n",
    "# sentences so that we can keep phrases together.\n",
    "# ngrams will help us pull out bigrams and trigrams from the\n",
    "# list of sentences.\n",
    "import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Download the stopwords list from NLTK.\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# We'll also use Pandas to create a dataframe (table) of n-grams.\n",
    "import pandas as pd\n",
    "\n",
    "# Open and read the corpus file.\n",
    "corpus = open(\"on_the_books_text_jc_all.txt\", \"r\").read()\n",
    "\n",
    "# Select the English list of stopwords from NLTK.\n",
    "# As with word frequencies, we want to avoid counting \n",
    "# n-grams of stopwords such as \"of the\" or \"and a\".\n",
    "stopWords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "# Create a list of words to add to NLTK's stopwords list. \n",
    "# This time, I have also added the newline character, '\\n' so that\n",
    "# it will not appear in the n-grams. \n",
    "# I've also added words such as \"identified,\" \"model,\" and \"expert.\"\n",
    "# ADD WORDS TO THIS LIST! Make sure they are lowercase, surrounded\n",
    "# by quote marks (' '), and separated from other words by\n",
    "# a comma.\n",
    "addedWords = ['identified','model','expert','shall','said','may','sec','general','assembly','north','carolina','state','act','ch','chapter','one','law','laws','private']\n",
    "\n",
    "# Add the new words to NLTK's existing stopwords list.\n",
    "stopWords.extend(addedWords)\n",
    "\n",
    "# Create an empty list to contain a list of n-grams.\n",
    "ngramsList = []\n",
    "\n",
    "# Instead of tokenizing individual words, we'll start by \n",
    "# using NLTK to tokenize (make a list of) sentences.\n",
    "sentences = nltk.sent_tokenize(corpus)\n",
    "\n",
    "# For each sentence in the tokenized sentences.\n",
    "for s in sentences:\n",
    "    \n",
    "    # Make all characters in the sentence lowercase.\n",
    "    s = s.lower()\n",
    "    \n",
    "    # Remove common punctuation and newline characters.\n",
    "    s = s.replace('.','')\n",
    "    s = s.replace(',','')\n",
    "    s = s.replace(':','')\n",
    "    s = s.replace(';','')\n",
    "    s = s.replace('\\n',' ')\n",
    "    s = s.replace('(','')\n",
    "    s = s.replace(')','')\n",
    "    s = s.replace('-','')\n",
    "    \n",
    "    # Tokenize the sentence by word. \n",
    "    # This creates a list of the words in one sentence.\n",
    "    wordSequence = word_tokenize(s)\n",
    "    \n",
    "    # Create an empty list to hold sentences after stopwords and \n",
    "    # punctuation have been removed.\n",
    "    sequence = []\n",
    "    \n",
    "    # Examine each word in the word sequence.\n",
    "    for word in wordSequence: \n",
    "        \n",
    "        # If the word does not appear in the stopwords list, \n",
    "        # add it to the sequence list.\n",
    "        if word not in stopWords:\n",
    "            sequence.append(word)\n",
    "        \n",
    "        # If the word appears in the stopwords list, skip it.\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    # From the sequence list, create a list of each two-word phrase.\n",
    "    # HINT: to get trigrams instead of bigrams, \n",
    "    # change the \"2\" below to \"3\".\n",
    "    ngramsList.extend(list(ngrams(sequence, 2)))\n",
    "\n",
    "print('N-grams list created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran the above script, you should now have a list of bigrams. We'll next add these to a dataframe so that we can preview the list. You'll be able to view the full list by running a second script below to save it to a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of times each bigram appears in the corpus.\n",
    "nFreq = nltk.FreqDist(ngramsList)\n",
    "\n",
    "# Make a new Pandas dataframe of bigrams and their counts for \n",
    "# easier reading.\n",
    "ngramDF = pd.DataFrame(list(nFreq.items()),columns = ['ngram','count']) \n",
    "\n",
    "# Sort the dataframe so that the most frequently occurring terms \n",
    "# appear first.\n",
    "ngramDF.sort_values(by=['count'], inplace=True, ascending=False)\n",
    "\n",
    "# Show us the first 20 entries in the dataframe.\n",
    "# We ask for 21 rows because the row with column names is included.\n",
    "ngramDF.head(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What do you notice about this list?*\n",
    "\n",
    "Because we removed stopwords *before* creating a list of two-word phrases (bigrams), we have been able to capture names such as \"board of education\" as \"board, education\" -- why might this be important? What kinds of issues might removing stopwords *before* creating bigrams introduce? If we wanted to create bigrams *and then* remove bigrams with stopwords, how might we do that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, use the following to save your bigrams to a .csv. Then, to get trigrams, change the above scripts -- look for the `# HINT`! Then change the file name in the script below to save your trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write this dataframe in a csv file. We'll use a pipe (|)\n",
    "# to separate cells for now to avoid interfering with \n",
    "# comma usage bigram column. We'll also set index to False, \n",
    "# which will exclude the row numbers in the far left column above \n",
    "# from the csv file.\n",
    "# HINT: change \"bigram\" to \"trigram\" below to save the \"trigramDF\"\n",
    "# as \"jclaws_trigrams.csv\".\n",
    "ngramDF.to_csv('jclaws_bigrams.csv', sep=\"|\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that we have a list of bigrams (two-word phrases), let's go back and create trigrams (three-word phrases). Look for the `# HINTS` in the scripts above to modify each script to gather trigrams instead of bigrams.**\n",
    "\n",
    "By now, we should be getting to a more meaningful list of word frequencies. What does this list tell us about the issues covered in this corpus? How were these issues legislated? What does frequency, as opposed to infrequency, tell us about these words' potential significance? To get a better sense of this, we need to see some of these words in context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How are commonly appearing words used in context?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What are some ways we could find frequent words in a corpus in order to view their context, or the way they are used in specific instances?* We could use the Find (Command+F) feature in a text editor to move through a .txt file and read each instance.\n",
    "\n",
    "*What if we want to be able to compare usages without having to move back and forth through the document?* This is where a <a href=\"https://en.wikipedia.org/wiki/Key_Word_in_Context\" target=\"blank\">keyword in context concordance (KWIC)</a> can be useful. If a concordance is a list of each time a word is used in a text, a KWIC provides the *context* around that word. We saw an example of a KWIC earlier in <a href=\"#voyant\">Voyant</a>. The format typically looks like this: \n",
    "\n",
    "<code>keep the names of the white voters <strong>separate</strong> and apart from those of colored vo</code>\n",
    "\n",
    "<code>d schools shall be in distinct and <strong>separate</strong> buildings and departments , and th</code>\n",
    "\n",
    "<code>ments , and the schools shall have <strong>separate</strong> apartments for the higher classes</code>\n",
    "\n",
    "<code>ping the names of the white voters <strong>separate</strong> and apart from those of colored vo</code>\n",
    "\n",
    "Each line of text represents a different instance in which a particular word, the keyword shown in bold, appears in the corpus. A number of characters (appoximately 35 here) preceding and following each keyword's appearance is also shown to provide the *context*. In some software programs, such as <a href=\"https://www.laurenceanthony.net/software/antconc/\" target=\"blank\">AntConc</a>, the location of each usage will also appear to the right of each line.\n",
    "\n",
    "Let's see how we can create a KWIC using NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Text module from NLTK.\n",
    "# The Text module contains a special data type (called \"Text\"),\n",
    "# which is essentially an NLTK container for strings (text).\n",
    "from nltk.text import Text\n",
    "\n",
    "# Open and read the corpus of Jim Crow laws.\n",
    "corpus = open(\"on_the_books_text_jc_all.txt\").read()\n",
    "\n",
    "# Tokenize the corpus.\n",
    "corpusTokens = word_tokenize(corpus)\n",
    "\n",
    "# Create an NLTK Text object to contain the tokenized corpus.\n",
    "corpusList = Text(corpusTokens)\n",
    "\n",
    "# Create and show us a concordance for a particular word.\n",
    "# TRY CHANGING THE WORD 'separate' to see what happens.\n",
    "# Change the width value to increase or decrease the number \n",
    "# of characters around each keyword.\n",
    "# Change the lines value to increase or descrease the number \n",
    "# of lines shown here.\n",
    "# Note that this function does not work with n-grams.\n",
    "corpusList.concordance('separate', width=50, lines=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What do you notice about this output? Try changing the keyword and rerunning the script a few times. Change the width (number of characters per line) and lines (number of lines printed) values to change the context's shape.*\n",
    "\n",
    "Note that the keyword in this output is not bolded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <p><strong>Questions to consider:</strong></p>\n",
    "    <ul>\n",
    "        <li>Why is context important?</li>\n",
    "        <li>How can we use word frequencies and KWICs together to explore a corpus?</li>\n",
    "        <li>What are the limitations of these methods both separately and together?</li>\n",
    "    </ul>\n",
    "    <p><strong>If you want to continue exploring topics &amp; themes:</strong></p>\n",
    "    <ul>\n",
    "        <li>You might have noticed that multiple words, n-grams, cannot be used as keywords. If you'd like to try this, check out <a href=\"https://programminghistorian.org/en/lessons/keywords-in-context-using-n-grams#from-text-to-n-grams-to-kwic\" target=\"blank\">this tutorial series on <em>The Programming Historian</em></a>.</li>\n",
    "        <li>Try out topic modeling with <a href=\"https://dariah-de.github.io/TopicsExplorer/\" target=\"blank\">DARIAH-DE's Topics Explorer</a>.</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time <a class=\"anchor\" id=\"time\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing textsf over time can be a useful way to develop a large-scale understanding of how a corpus developed. In this instance, we can use time (volume years, in this dataset) to begin placing Jim Crow laws in temporal context. Based on answers to the following question, we could then think about the kinds of events that caused or were caused by the passage of these laws:\n",
    "\n",
    "- How many laws per year have been identified?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many laws per year have been identified? <a class=\"anchor\" id=\"laws-per-year\"></a>\n",
    "\n",
    "Earlier, we looked at the [number of laws per volume](#laws-per-volume) identified in our dataset. We used the volume year(s) to label each volume and then calculated the number of Jim Crow laws identified per volume year. Let's review this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show us the volumesDF dataframe.\n",
    "volumesDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What do you notice about this list of years? Is every year between 1865-1966 included? Are only individual years included?*\n",
    "\n",
    "In the earlier volumes from the 1860s and 1870s, you may notice that some years are listed as, for example, \"1874/75\". Let's look at <a href=\"https://archive.org/details/lawsresolutionso187475nor\" target=\"blank\">this volume's title page</a> to see why this is the case:\n",
    "\n",
    "<img src=\"images/10-explore-01.jpeg\" width=\"90%\" style=\"margin:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Detail of the title page for the North Carolina session laws volume 18754-1875.\" title=\"Detail of the title page for the North Carolina session laws volume 1874-1875.\"/>\n",
    "\n",
    "For this volume, the state's legislative session was not held within one calendar year but across two: the session began in November 1874 and concluded in March 1875. (Although not written on this page, this information is publicly available on [Wikipedia](https://en.wikipedia.org/wiki/List_of_North_Carolina_state_legislatures) and can also be found in the first and last chapters of the volume.) To answer our current question, do we need to assign the laws from volumes such as 1874/75 to a single year? If our aim is to show the number of laws per year, and sessions overlapping multiple years were begun *late* in the first year (e.g. November 1874), then one option would be to choose the latter year, 1875 in this case. This choice would align with the printer's year on the volume's title page, and it would cover all laws passed from November 1874 until the end of session in 1875. Whereas at the start of the 1874 session, none of the laws in the volume had been passed. The choice would also align with historic patterns for holding legislative sessions, which are described as follows:\n",
    "\n",
    "<blockquote><p>Legislative sessions generally began in November of odd years until 1875, when the opening day shifted to the first Wednesday after the first Monday in January after an election. From 1957 to 1967 sessions were convened in February, but since 1967 they have begun in January. A 1989 law specifies that the General Assembly convenes at noon on \"the third Wednesday after the second Monday in January after the election.\" From the eighteenth century until well into the twentieth, the \"long\" sessions held in odd years generally lasted from one to three months, depending on the amount of business pending. In recent years, sessions have lasted for six or seven months. The General Assembly now also holds a \"short session\" in even-numbered years to adjust the budget and deal with other necessary matters.</p>\n",
    "    <p>~ Norris, David A. \"General Assembly.\" <em>NCPedia.</em> 2006. Accessed April 2, 2021. <a href=\"https://www.ncpedia.org/general-assembly\" target=\"blank\">https://www.ncpedia.org/general-assembly</a>.</p></blockquote>\n",
    "\n",
    "So we can assume that most sessions were held, or finished, in an odd year. Those that were held or finished in an even year were most likely special sessions. We'll opt to label all volume years formatted as \"1874/75\" by the latter year, in this case 1875. We can do this by expanding the volumes dataframe to include a specific year without changing the volume column itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# First, we'll create a new dataframe for years out of volumesDF.\n",
    "yearsDF = volumesDF\n",
    "\n",
    "# Next, we'll copy the data in the Volumes column into a new \n",
    "# \"Year\" column.\n",
    "yearsDF['Year'] = yearsDF['Volume']\n",
    "\n",
    "# Now, we'll go through each entry in the \"Year\" column. \n",
    "for year in yearsDF['Year']:\n",
    "    \n",
    "    # If an entry contains a \"/\" -- e.g. \"1874/75\",\n",
    "    # we'll change the entry to show only the later year (e.g. \"1875\").\n",
    "    if '/' in year:\n",
    "        \n",
    "        # Let's get the first 2 digits in the year, which won't change.\n",
    "        century = re.search('\\d\\d', year)\n",
    "        \n",
    "        # Now, let's get the 2 digits following the \"/\".\n",
    "        # In the regular expression below, \"(?<=/)\" is a pattern \n",
    "        # that essentially tells Python to look for the \"/\" and \n",
    "        # to match the digits (\"\\d\\d\") after it.\n",
    "        decade = re.search('(?<=/)\\d\\d', year)\n",
    "        \n",
    "        # Let's combine century and decade to create a new year value.\n",
    "        newYear = century.group() + decade.group()\n",
    "        \n",
    "        # And now let's replace the existing contents of the Year \n",
    "        # cell with our new year value.\n",
    "        yearsDF['Year'] = yearsDF['Year'].replace([year], newYear)\n",
    "    \n",
    "    # If an entry does not contain a \"/\", move to the next year.\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# Show a preview of the yearsDF dataframe.\n",
    "yearsDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should have a third column in our years dataframe that includes only a single year for each volume. For volumes that are listed with only one year, the same year should appear in both the Volume and Year columns.\n",
    "\n",
    "We can now visualize this on a chart, which could look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that the data are sorted by year.\n",
    "yearsDF = yearsDF.sort_values(by='Year')\n",
    "\n",
    "# Plot the number of laws per year in a line chart.\n",
    "# Include a marker to show specific data points.\n",
    "# Also make the graph wider for easier reading.\n",
    "yearsDF.plot(x ='Year', y='Count', kind = 'line', marker='o', figsize=(20,6))\n",
    "\n",
    "# Show us the resulting chart.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier, we considered what the limitations of this chart might be: Because there are so few years labeled and because the line is continuous, we may be inclined to think that *some* Jim Crow laws were passed every year between 1865-1966. We added a circular marker in the graph above to show specific data points and made the graph wider to try to make it easer to read the data. But it still appears as if law passage was *continuous*, and it's hard for us to see actual values and dates.\n",
    "\n",
    "Remember that we know North Carolina's general assembly usually only convened during *odd* years, and many even years have not been included in our years dataframe. Instead of a continuous line, we could show this information as a bar chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that the data are sorted by year.\n",
    "yearsDF = yearsDF.sort_values(by='Year')\n",
    "\n",
    "# Plot the number of laws per year as a bar chart.\n",
    "# Use the \"figsize\" parameter to assign a width to the chart, making\n",
    "# each bar label more legible.\n",
    "yearsDF.plot(x ='Year', y='Count', kind = 'bar', figsize=(20, 6))\n",
    "\n",
    "# Show us the resulting chart.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see each individual year and can note when the highest and lowest numbers of laws were passed. This may be sufficient, but what if we also want to show the years *not included* because the general assembly was not in session? We can add those years by creating a list of *all* years between 1865-1966 (ending with 1966 so as to *include* 1965); comparing the years in our dataframe to that list; and, if a year is missing, adding it to the years dataframe.\n",
    "\n",
    "First, let's create our list of years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a start value.\n",
    "startYear = 1865\n",
    "\n",
    "# Set an end value.\n",
    "endYear = 1966\n",
    "\n",
    "# Create an empty list.\n",
    "yearsList = []\n",
    "\n",
    "\n",
    "# While the start value is less than the end value:\n",
    "while startYear < endYear:\n",
    "    \n",
    "    # Add the start value to the list.\n",
    "    yearsList.append(startYear)\n",
    "    \n",
    "    # Increase the start value by one to get the next year in sequence.\n",
    "    startYear += 1\n",
    "\n",
    "# Show us the completed years list.\n",
    "print(yearsList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare our years dataframe to this list and add missing years into the dataframe. For these added years, we'll include a value \"no volume\" for the Volumes column and \"0\" for the Count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each year in the list of years 1865-1966:\n",
    "for year in yearsList:\n",
    "    \n",
    "    # Check whether the year appears in the years dataframe\n",
    "    # \"Year\" column. If the year is not there, then do the following: \n",
    "    if str(year) not in yearsDF['Year'].values:\n",
    "\n",
    "        # Create a new row for the year. \n",
    "        # The \"Volume\" column will contain \"no volume\".\n",
    "        # \"Count\" should be set to \"0\".\n",
    "        # And \"Year\" should be set to the current year in the list.\n",
    "        # Make sure the year is added as a string (text) value\n",
    "        # and not as a number (integer).\n",
    "        newRow = {'Volume':'no volume', 'Count':0, 'Year':str(year)}\n",
    "        \n",
    "        # Add the new row to the years dataframe.\n",
    "        yearsDF = yearsDF.append(newRow, ignore_index=True)\n",
    "    \n",
    "    # If the year in yearsList IS found in the years dataframe \n",
    "    # \"Year\" column, then move on to the next year in the list.\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# Resort the data by year.\n",
    "yearsDF = yearsDF.sort_values(by='Year')\n",
    "\n",
    "# Show us a preview of the years dataframe.\n",
    "yearsDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this new dataset, with all years 1865-1966 included, to create a chart showing years when the general assembly was not in session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of laws per year as a bar chart.\n",
    "# Use the \"figsize\" parameter to assign a width to the chart, making\n",
    "# each bar label more legible. Also use \"width\" to specify the width\n",
    "# of each bar to ensure consistency.\n",
    "yearsDF.plot(x ='Year', y='Count', kind = 'bar', figsize=(20, 6), width=0.8)\n",
    "\n",
    "# Show us the resulting chart.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What can we learn from this bar chart? What can we **not** learn from it? How might we continue to improve it?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <strong>How else can we think about time?</strong> \n",
    "    <p>Building on work in some of the other sections of this module, we might also ask</p>\n",
    "        <ul>\n",
    "            <li>Can themes or topics be traced temporally, e.g. does one topic or theme appear in the laws primarily within a few years, or is it dispursed across the corpus?</li>\n",
    "            <li>Can we see any spatial patterns over time?</li>\n",
    "    </ul>\n",
    "    <p><strong>Consider:</strong> how would you approach these questions? How would you need to modify the dataset to answer these questions? What might be the steps you would take in Python to answer these questions?</p>\n",
    "    <p>Are there other questions that come to mind? How might you answer them?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Space <a class=\"anchor\" id=\"space\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with time, analyzing spatial distribution--the specific locations mentioned in Jim Crow laws--can help us get a big picture view of the dataset. From there, we might choose a specific region to research in order to better understand what was happening in that area. In this module, though, we'll start by visualizing:\n",
    "\n",
    "- How many laws were passed at a local level? At a state level?\n",
    "- Where in North Carolian were Jim Crow laws adopted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many laws were passed at a local level? At a state level?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In *Jim Crow in North Carolina: The Legislative Program from 1865-1920*, lawyer historian Richard A. Paschal identified and analyzed laws that produced and perpetuated racial discrimination and violence. Of the laws found, Paschal notes \n",
    "\n",
    "> \"many laws were drafted so as to be limited in application to one town or county and, thus, there were few laws of general applicability. [...] Perhaps one possible reason for a limited scope in an initial statute is that localities might be alowed to try something new [...]\" (Paschal 121)\n",
    "\n",
    "Does this observation hold up in the *On The Books* dataset? We can start to find out by counting the number of laws that are applicable at the state as opposed to a local--town, city, or county--level. How will we figure this out? Let's take a look again at the dataset--specifically the ChapterTitle column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://pythonexamples.org/pandas-dataframe-read-csv-load-data-csv/#2\n",
    "\n",
    "# Import Pandas again just in case.\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Jim Crow laws dataset.\n",
    "df = pd.read_csv('jclaws_dataset.csv', delimiter='|')\n",
    "\n",
    "# Set the number of characters to display to 150.\n",
    "# CHANGE 150 below to a larger or smaller number to see more\n",
    "# or less text.\n",
    "pd.options.display.max_colwidth = 150\n",
    "\n",
    "# Display just the first 30 rows in the ChapterTitle column.\n",
    "# CHANGE 'ChapterTitle' below to 'SectionText' to view the \n",
    "# section text for a particular law.\n",
    "df['ChapterTitle'].head(31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above should show you just the first 30 rows of the ChapterTitle column and most or all of the text for each. **What do you notice about these chapters? Are there any noticable patterns in the chapter titles' structures?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're looking at the first 30 because, as you may have noticed, some chapter titles repeat because multiple sections from within those chapters have been classified as Jim Crow laws. In spite of these repeats, you may have noticed a few things about how these are written:\n",
    "\n",
    "- Chapter titles typically begin with `AN ACT TO` and then verbs such as `AUTHORIZE`, `ESTABLISH`, `DONATE`, `REVISE AND CONSOLIDATE`.\n",
    "\n",
    "- Then the object of the law--what the law will be applied to--is given: `PUBLIC SCHOOL LAW`, `NORMAL SCHOOLS`, `CHARTER`, `CERTAIN COLORED PERSONS`. \n",
    "\n",
    "- And then some boundaries, often geographic, are set on those objects: `IN GOLDSBORO IN WAYNE COUNTY`, `THE CITY OF CHARLOTTE`, `WASHINGTON, NORTH CAROLINA`.\n",
    "\n",
    "This last point is especially important: town and city names typically (though not always) begin with `TOWN OF` or `CITY OF`. Meanwhile, county names are typically followed by `COUNTY`. \n",
    "\n",
    "**Revise the code above (look for the `#CHANGE` hints) to view the SectionText column and see if the town, city, and county names follow this pattern. Do you notice any differences?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an initial sense of how many Jim Crow local and state laws there are in the *On The Books* dataset, we'll focus on the ChapterTitle column. The following code searches the `ChapterTitle` column for the words `town`, `city`, and `county` -- and the plurals of `city` and `county`. Titles that include these terms are classified as `local`; titles that do not include these terms are classifed as `state`, and these classifications are added to a new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Create a list of variables to search for.\n",
    "town = 'town'\n",
    "city = 'city'\n",
    "cities = 'cities'\n",
    "county = 'county'\n",
    "counties = 'counties'\n",
    "\n",
    "# Create variables to classify laws as local or state.\n",
    "local = 'local'\n",
    "state = 'state'\n",
    "\n",
    "# Go through each row in the dataframe.\n",
    "for i,row in df.iterrows():\n",
    "    \n",
    "    # Get the chapter title in a specific row.\n",
    "    chapter = str(df.loc[i,'ChapterTitle'])\n",
    "    \n",
    "    # Make the chapter title all lowercase.\n",
    "    chapter = chapter.lower()\n",
    "    \n",
    "    # The following statements search for each of the variables above\n",
    "    # in one ChapterTitle and assigns the corresponding law the 'local' \n",
    "    # Location Type if one of the variables is found. If none of the \n",
    "    # variables are found in that ChapterTitle, the Location Type for\n",
    "    # that law is set to 'state'. Then Python returns to the 'for'\n",
    "    # statement above, moves to the next ChapterTitle and repeats this\n",
    "    # process.\n",
    "    \n",
    "    # Search a ChapterTitle for 'city'.\n",
    "    if re.search(city, chapter):\n",
    "        \n",
    "        # If 'city' was found, set LocationType to 'local'.\n",
    "        # If 'city' was NOT found, move on to the next statement.\n",
    "        df.loc[i,'LocationType'] = local\n",
    "    \n",
    "    \n",
    "    # 'elif' is short for 'else if'. If the statement above did not\n",
    "    # return a result ('city' was not found) then Python will run\n",
    "    # the statement. If that statement is false ('citites' is not found), \n",
    "    # it will run the next 'elif' statement after that. If 'cities' is\n",
    "    # found (the statement is TRUE), then Python will move on to the next\n",
    "    # ChapterTitle.\n",
    "    \n",
    "    # Search a ChapterTitle for 'cities'.\n",
    "    elif re.search(cities, chapter):\n",
    "\n",
    "        # If 'cities' was found, set LocationType to 'local'.\n",
    "        # If 'cities' was NOT found, move on to the next statement.        \n",
    "        df.loc[i,'LocationType'] = local\n",
    "    \n",
    "    # Search a ChapterTitle for 'town'.\n",
    "    elif re.search(town, chapter):\n",
    "        \n",
    "        # If 'town' was found, set LocationType to 'local'.\n",
    "        # If 'town' was NOT found, move on to the next statement.  \n",
    "        df.loc[i,'LocationType'] = local\n",
    "        \n",
    "    # Search a ChapterTitle for 'county'.\n",
    "    elif re.search(county, chapter):\n",
    "        \n",
    "        # If 'county' was found, set LocationType to 'local'.\n",
    "        # If 'county' was NOT found, move on to the next statement.  \n",
    "        df.loc[i,'LocationType'] = local\n",
    "    \n",
    "    # Search a ChapterTitle for 'counties'.\n",
    "    elif re.search(counties, chapter):\n",
    "        \n",
    "        # If 'counties' was found, set LocationType to 'local'.\n",
    "        # If 'counties' was NOT found, move on to the next statement.  \n",
    "        df.loc[i,'LocationType'] = local\n",
    "    \n",
    "    # If none of the statements above are true (if none of the \n",
    "    # variables were found), then set the LocationType to 'state'.\n",
    "    else:\n",
    "        \n",
    "        df.loc[i,'LocationType'] = state\n",
    "\n",
    "# OPTIONAL: Remove the comment # below to save this file as a new .csv.\n",
    "# df.to_csv(r'jclaws_dataset_locationtype.csv', index=False, sep='|')\n",
    "\n",
    "# Display the first 50 laws in the dataset.\n",
    "df.head(51)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Before we examine the results, let's consider the code above:*\n",
    "\n",
    "- Why did we use variables to list out the search terms?\n",
    "- Why did we *not* need to include a variable for the plural of `town`?\n",
    "- Were there other search terms you might have added?\n",
    "- Based on your earlier reading of the chapter titles, does the search logic make sense? If not, how would you have searched differently?\n",
    "\n",
    "*Now let's consider the preview of results above. Read through the chapter titles and their LocationType classifications. Are these classifications correct?* Have you noticed any ChapterTitles that have been misidentified as `state` or `local`? Why do you think they were misidentified? \n",
    "\n",
    "Here is one example:\n",
    "\n",
    "The 1879 Public Law Ch. 308 Section 1 Chapter Title reads (with OCR errors corrected): \n",
    "\n",
    "`CHAPTER 308 AN ACT TO CHARTER THE OCEAN FIRE COMPANY, OF WASHINGTON, NORTH CAROLINA.`\n",
    "\n",
    "In this example, town is named, but there is now `TOWN OF` or `CITY OF` included in the chapter title. Instead, we have only the town name followed by the state name `WASHINGTON, NORTH CAROLINA`. Because there was no `TOWN OF` or `CITY OF`, our code has classified the LocationType as `state`. How might we address exceptions such as these to avoid misidentifying laws?\n",
    "\n",
    "One way would be to add a variable that searches for `, NORTH CAROLINA`, the common way of naming a state after a city or town. We could insert above a variable such as:\n",
    "\n",
    "`comma_nc = ', north carolina'`\n",
    "\n",
    "And we could add an `elif` statement that searches for that term and classifies it as `local`:\n",
    "   \n",
    "    # Search a ChapterTitle for ', north carolina'.\n",
    "    elif re.search(comma_nc, chapter):\n",
    "        \n",
    "        # If ', north carolina' was found, set LocationType to 'local'.\n",
    "        # If ', north carolina' was NOT found, move on to the next statement.  \n",
    "        df.loc[i,'LocationType'] = local      \n",
    "\n",
    "**Try adding the above lines into the code and rerunning the script. Where should they be inserted?** If you rerun the script and receive an error, check that \n",
    "- the variable is named *before* the `elif` statement,\n",
    "- the `elif` statement appears before `else`,\n",
    "- and the `elif` statement and `df.loc` are correctly indented (their indentation should match the other `elif` statements' indentations).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's count how many state and local laws we found:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the dataframe (df) column \"LocationType,\"\n",
    "# count all of the unique values ('state' and 'local').\n",
    "df['LocationType'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do these results align with Paschal's observation? We can create a graph to get another look. As we did in the [Quantitative](#quantitative) section above, let's create a new Pandas dataframe out of the results above and then create a bar graph using Matplotlib: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we'll need to create a new dataframe from the LocationType \n",
    "# counts above:\n",
    "\n",
    "# Store the list of values created above in a variable called \n",
    "# \"valueCounts\".\n",
    "valueCounts = df['LocationType'].value_counts()\n",
    "\n",
    "# Create a new dataframe, \"LocationTypeDF,\" using valueCounts.\n",
    "LocationTypeDF = pd.DataFrame(valueCounts)\n",
    "\n",
    "# Set the row numbers (index) for the new dataframe.\n",
    "LocationTypeDF = LocationTypeDF.reset_index()\n",
    "\n",
    "# Create the column names for the IdentifiedBy dataframe.\n",
    "LocationTypeDF.columns = ['LocationType', 'Count']\n",
    "\n",
    "# Show a preview of the IdentifiedBy dataframe.\n",
    "LocationTypeDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have our new dataframe, we can create a bar chart:\n",
    "\n",
    "# We just need one part of matplotlib, the pyplot module.\n",
    "# We'll import pyplot and call it \"plt\" for short.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assign the x and y axes to specific columns.\n",
    "LocationTypeDF.plot(x ='LocationType', y='Count', kind = 'bar')\n",
    "\n",
    "# Show us the resulting bar chart.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <strong>For further exploration:</strong>\n",
    "    <p>Based on the above, it does seem as though there <em>were</em> a significant number of state-wide Jim Crow statutes passed into law. If we wanted to take this a step further, we could see when these state-wide laws were created: Paschal's research stops with 1920, and the <em>On The Books</em> dataset continues through 1967.</p> \n",
    "        <ul>\n",
    "            <li>So were these laws passed <em>after</em> 1967?</li>\n",
    "            <li>Has <em>On The Books</em> identified laws as producing or perpetuating Jim Crow that Paschal passed over?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where in North Carolian were Jim Crow laws adopted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've identified which laws were applied only to local areas, but what if we wanted to see *which* local areas were subject to Jim Crow laws passed by the North Carolina General Assembly? What if we wanted to map the number of laws passed in specific local areas?\n",
    "\n",
    "There are a number of ways we could approach these questions. We've already seen how we can use the formulaic language in chapter titles, and we could return to this to classify laws by their location rather than a location *type*. Remember that we've seen at least 3 different types of location specific to smaller geographic areas: town, city, and county. **For this exercise, we're going to look at only 1 location type: county.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with a script that will look similar to the one above that we used to search for location types. This time, however, we'll look only for `county` and `counties` to create a new counties dataframe that we'll use for mapping. \n",
    "\n",
    "*This search will exclude mentions of cities, towns, and other smaller local areas. Those laws would need to be identified and mapped separately. So the results we produce here will not include all laws within county boundaries but just those laws applied at the county level, or laws which mention the county name in the chapter title.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Create a list of variables to search for.\n",
    "county = 'county'\n",
    "counties = 'counties'\n",
    "\n",
    "# Create a new dataframe, \"countiesDF\".\n",
    "# Set the columns in the new dataframe to match those in our main \n",
    "# Jim Crow laws dataframe (df).\n",
    "countiesDF = pd.DataFrame(columns=['VolumeLawTypeChapterSection','IdentifiedBy','ChapterTitle','SectionText','Volume','LawType','ChapterNum.','SectionNum.','LocationType'])\n",
    "\n",
    "# Go through each row in the original dataframe.\n",
    "for i,row in df.iterrows():\n",
    "    \n",
    "    # Get the chapter title in a specific row.\n",
    "    chapter = str(df.loc[i,'ChapterTitle'])\n",
    "    \n",
    "    # Make the chapter title all lowercase.\n",
    "    chapter = chapter.lower()\n",
    "\n",
    "    # Search a ChapterTitle for 'county'.\n",
    "    if re.search(county, chapter):\n",
    "        \n",
    "        # If 'county' was found, add the law to the new countiesDF.\n",
    "        # If 'county' was NOT found, move on to the next statement.\n",
    "        countiesDF = countiesDF.append(row, ignore_index=True)\n",
    "    \n",
    "    # Search a ChapterTitle for 'counties'.\n",
    "    elif re.search(counties, chapter):\n",
    "        \n",
    "        # If 'counties' was found, add the law to the new countiesDF.\n",
    "        # If 'counties' was NOT found, move on to the next statement.\n",
    "        countiesDF = countiesDF.append(row, ignore_index=True)\n",
    "    \n",
    "    # If none of the statements above are true (if none of the \n",
    "    # variables were found), then skip this law, don't add it\n",
    "    # to countiesDF, and move on to the next law.\n",
    "    else:\n",
    "        \n",
    "        continue\n",
    "\n",
    "# OPTIONAL: Remove the # below to save this file as a new .csv.\n",
    "# countiesDF.to_csv(r'jclaws_dataset_counties.csv', index=False, sep='|')\n",
    "\n",
    "# Display the first 50 laws in the dataset.\n",
    "countiesDF.head(51)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Look through the results above. What do you notice about the laws that have been selected?**\n",
    "\n",
    "You may have noticed that some chapter titles name towns or townships as well as counties. These laws would have applied only to those specific locations *within* the counties, but we'll include them in our counties dataset because some part of the county would have been subject to those laws.\n",
    "\n",
    "**Our next step will be to identify the names of specific counties within the counties datatframe.** You might wonder why we didn't start with this step: some North Carolina towns/cities and counties share the same name. For our purposes, it is simpler to pull out only laws that include `COUNTY` in the chapter title. This should minimize instances of locations being misidentified as counties. \n",
    "\n",
    "To create this new dataframe and ultimately map it, we'll follow these steps:\n",
    "\n",
    "1. [*Get a list of county names.*](#1) Today, North Carolina has 100 counties. We're working with [this list of counties maintained by the UNC School of Government's Knapp Library](https://www.sog.unc.edu/resources/microsites/knapp-library/counties-north-carolina). \n",
    "\n",
    "    This list accounts only for *current counties*. During the period covered by the *On The Books* dataset, some county names were changed and boundaries shifted. For example, Durham County was carved out of Orange and Wake Counties in 1881. So any laws passed prior to 1881 that would have pertained to the area now known as Durham County will be classified as belonging to Orange or Wake Counties respectively. Further research on those laws and if/how they would have applied to Durham County would need to be done in order to create a more historically accurate spatial dataset. \n",
    " \n",
    " \n",
    "2. [*Create a new dataframe to hold the laws and their associated county names.*](#2) We're doing this because there are instances of chapter titles containing multiple county names, for example: `CHAPTER 330 An act to establish graded schools in the counties of Nash and Edgecombe.`\n",
    "\n",
    "    This law and others like it will need to be mapped multiple times to ensure that both Nash and Edgecombe counties are represented. Therefore, in our new dataframe, a laws pertaining to multiple counties will be replicated to account for each county listed. This, the section associated with `CHAPTER 330` above will appear once with `Nash` in a new `CountyName` column and once with `Edgecome` in the `CountyName` column. We want to avoid introducing this duplication into our main Jim Crow laws dataframe, which is why we are working in a separate dataframe.\n",
    "\n",
    "\n",
    "3. [*Use the list of county names to search the chapter title.*](#3) As described above, for each county name found, we'll append the law with it's associated county name(s) to the new dataframe. \n",
    "\n",
    "\n",
    "4. [*Count the number of laws per county.*](#4) We'll put this into its own dataframe for mapping.\n",
    "\n",
    "\n",
    "5. [*Map the number of laws per county.*](#5) We'll use [geopandas](https://geopandas.org/) to create the map and will use a color gradient to indicate the number of laws per county. To do this, we'll need to\n",
    "\n",
    "    - Import geographic county shapes, or polygons stored in a GIS format called a [shapefile](https://en.wikipedia.org/wiki/Shapefile).\n",
    "    - Match the county names in our laws-per-county dataframe to the county shapes.\n",
    "    - Use matplotlib and geopandas to create a [choropleth map](https://en.wikipedia.org/wiki/Choropleth_map) to show us the number of laws per county in a geographic visualization (map)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get a list of county names. <a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "We've provided a [list of county names](NC_counties.txt) that we'll need to pull into Python in order to search for each of them in the chapter titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of searching individual terms as we did for county, town, \n",
    "# etc., above, we'll create a list of county names and search the \n",
    "# Jim Crow laws dataset for each name in the list.\n",
    "# The following creates an empty list that we'll fill from a file.\n",
    "counties_list = []\n",
    "\n",
    "# Open a text file that contains each county name\n",
    "# on a separate line. Note that \"County\" is not included with each name.\n",
    "with open('NC_counties.txt', 'r') as file:\n",
    "    \n",
    "    # For each county name in the file.\n",
    "    for county in file:\n",
    "        \n",
    "        # Remove the invisible escape character at the end of each name.\n",
    "        county = county.strip('\\n')\n",
    "        \n",
    "        # Add the county name from the file to our counties_list list.\n",
    "        counties_list.append(county)\n",
    "\n",
    "# Display the list, which should now include all 100 counties.\n",
    "print(counties_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a new dataframe to hold the laws and their associated county names. <a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "This is an important step to complete because we will need to make some changes to the data structure--add rows for laws that are applied to multiple counties, and add another column. We don't want to alter our initial dataset, and we only need to work with the laws that we've already filtered into the `countiesDF` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new, empty dataframe. \n",
    "# Note that we are excluding some of the columns that we're not\n",
    "# working with to simplify the dataframe. \n",
    "# There may be reasons you would want to include them, for example,\n",
    "# to map which types of laws (public or private) are pass where.\n",
    "geoDF = pd.DataFrame(columns=['ChapterTitle','SectionText','Volume','ChapterNum.','SectionNum.','CountyName'])\n",
    "\n",
    "# Display the column names and empty dataframe.\n",
    "geoDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Use the list of county names to search chapter titles. <a class=\"anchor\" id=\"3\"></a>\n",
    "\n",
    "We'll now use our `counties_list` to search Chapter Titles in `countiesDF` for county names and add those laws to the new `geoDF` with county names included in the `CountyName` column. Laws that include multiple county names will be replicated for each county name--so if 2 counties are named in one law's Chapter Title, that law will appear twice, once for each of the county names.\n",
    "\n",
    "It may take a few minutes for Python and Binder to complete this task. Remember that there are 100 counties in North Carolina, and we're asking Python to read through the `countiesDF` for each county (100 times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each county in the counties list.\n",
    "for county in counties_list:\n",
    "    \n",
    "    # Make the county name all lowercase.\n",
    "    county = county.lower()\n",
    "    \n",
    "    # Go through each row in countiesDF.\n",
    "    for i,row in countiesDF.iterrows():\n",
    "        \n",
    "        # Get the chapter title in the specific row.\n",
    "        chapter = str(countiesDF.loc[i,'ChapterTitle'])\n",
    "    \n",
    "        # Make the chapter title all lowercase.\n",
    "        chapter = chapter.lower()\n",
    "        \n",
    "        # If the county name is found in the chapter title...\n",
    "        if re.search(county,chapter):\n",
    "            \n",
    "            # Create a new row to add to the geoDF. \n",
    "            # Remember that we're only pulling in certain columns.\n",
    "            # This line will help us get and restructure each \n",
    "            # law's contents to fit these columns.\n",
    "            # Note that the very last column, County, pulls in the\n",
    "            # county name from counties_list.\n",
    "            newRow = {'ChapterTitle':countiesDF.loc[i,'ChapterTitle'], 'SectionText':countiesDF.loc[i,'SectionText'], 'Volume':countiesDF.loc[i,'Volume'], 'ChapterNum.':countiesDF.loc[i,'ChapterNum.'], 'SectionNum.':countiesDF.loc[i,'SectionNum.'], 'CountyName':str(county)}     \n",
    "        \n",
    "            # Add the new row to the years dataframe.\n",
    "            geoDF = geoDF.append(newRow, ignore_index=True)\n",
    "\n",
    "        # If the county name is NOT found, move on to the next county name.    \n",
    "        else:\n",
    "            continue \n",
    "\n",
    "# Display a preview of the new dataframe.\n",
    "geoDF.head(51)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Read through some of the chapter titles above. Have their county names been identified correctly?* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Count the number of laws per county. <a class=\"anchor\" id=\"4\"></a>\n",
    "\n",
    "Our new `geoDF` is great, but let's zoom out to get a bigger picture by counting the number of laws per county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of laws identified per county.\n",
    "geoCounts = geoDF['CountyName'].value_counts()\n",
    "\n",
    "# Create a new dataframe to hold the number of laws (count) per county.\n",
    "geoCountsDF = pd.DataFrame(geoCounts)\n",
    "\n",
    "# Set the row numbers (index) for the new dataframe.\n",
    "geoCountsDF = geoCountsDF.reset_index()\n",
    "\n",
    "# Create the column names for the IdentifiedBy dataframe.\n",
    "geoCountsDF.columns = ['CountyName', 'Count']\n",
    "\n",
    "# OPTIONAL: Remove the # from the line below to sort the geoCountsDF by county name.\n",
    "#geoCountsDF = geoCountsDF.sort_values(by='CountyName')\n",
    "\n",
    "# Remove the maximum limit on number of rows to display\n",
    "# so that we can see the full dataframe.\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Display a preview of the number of laws per county.\n",
    "# Note that we should see the number of rows at the bottom\n",
    "# of the output below -- this will let us know how\n",
    "# many of the counties have been identified.\n",
    "display(geoCountsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above table is great, but not all 100 counties appear because some were not identified in chapter titles. For mapping purposes, we may want to be able to produce a map that explicitly labels each county and the number of Jim Crow laws associated with it, even if that number if 0. To do this, we need to create a dataframe that includes both the counts above and the counties with 0 counts. \n",
    "\n",
    "In the following scripts, we'll create a second dataframe, `geoCountsAllDF` that we'll compare with the original `geoCountsDF` and use to create a complete dataframe. As you read, observe carefully which dataframe is being invoked when and how it's being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe to hold the number of laws (count) per county\n",
    "# as well as ALL counties for whom the count is 0.\n",
    "geoCountsAllDF = pd.DataFrame(columns=['CountyName','Count'])\n",
    "\n",
    "# We'll go back to the original counties_list and add\n",
    "# each county name to geoCountsAllDF.\n",
    "for county in counties_list:\n",
    "    \n",
    "    # Make the county name all lowercase.\n",
    "    county = county.lower()\n",
    "\n",
    "    # Create a new row to add to geoCountsAllDF. \n",
    "    # This row will include the county name and a count of 0.\n",
    "    # We'll add the values from geoCountsDF in next.\n",
    "    newRow = {'CountyName':county, 'Count':0}     \n",
    "\n",
    "    # Add the new row to the years dataframe.\n",
    "    geoCountsAllDF = geoCountsAllDF.append(newRow, ignore_index=True)\n",
    "\n",
    "geoCountsAllDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of our counties in a dataframe, let's check `geoCountsDF` against `geoCountsAllDF`. Whenever we find a count value greater than 0 for a given county in `geoCountsDF`, we'll replace the corresponding county's `Count` value in `geoCountsAllDF`. This should give us a more complete dataframe that includes all county names and values, including 0 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through each row in geoCountsAllDF \n",
    "# (the dataframe of all 100 counties with 0 Counts).\n",
    "for i,row in geoCountsAllDF.iterrows():\n",
    "\n",
    "    # Get the county name in the specific row.\n",
    "    countyName = str(geoCountsAllDF.loc[i,'CountyName'])\n",
    "\n",
    "    # Go through each row in geoCountsDF\n",
    "    # (the dataframe of only counties with Jim Crow laws\n",
    "    # identified in chapter titles).\n",
    "    for k,row in geoCountsDF.iterrows():\n",
    "    \n",
    "        # If the county name in geoCountsAllDF is present in geoCountsDF,\n",
    "        # move to the next if statement below, otherwise, continue to the\n",
    "        # else statement.\n",
    "        if countyName in geoCountsDF.loc[k,'CountyName']:\n",
    "            \n",
    "            # If the corresponding county name's Count in geoCountsDF is greater\n",
    "            # than the Count value in geoCountsAllDF (0)...\n",
    "            if geoCountsDF.loc[k,'Count'] > geoCountsAllDF.loc[i,'Count']:\n",
    "                \n",
    "                # Replace the value (0) in the geoCountsAllDF Count column\n",
    "                # with the value (non-zero) in geoCountsDF.\n",
    "                geoCountsAllDF.at[i,'Count'] = geoCountsDF.loc[k,'Count']\n",
    "        \n",
    "        # If the county name in geoCountsAllDF if NOT present in geoCountsDF,\n",
    "        # continue on to the next county name in geoCountsAllDF. The\n",
    "        # count value will remain at 0. This is an important step for ensuring\n",
    "        # that counties with 0 identified laws are included in our final\n",
    "        # dataframe of all counties.\n",
    "        else:\n",
    "            \n",
    "            continue\n",
    "\n",
    "# OPTIONAL: Remove the # from the line below to sort the geoCountsAllDF by Count.\n",
    "#geoCountsAllDF = geoCountsAllDF.sort_values(by='Count')\n",
    "\n",
    "# Display the final dataframe that includes all 100 county names and the number\n",
    "# of Jim Crow laws identified in that county.\n",
    "geoCountsAllDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a dataset that include *all* county names and *all* values, including 0, we can move forward with our spatial analysis:\n",
    "\n",
    "Another way we can get a sense of how many Jim Crow laws were identified per county is by visualizing them. We can do this as we've done previously with a simple bar graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just in case, we'll reimport matplot lib's pyplot module.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# OPTIONAL: Remove the # from the line below to sort the geoCountsAllDF by Count.\n",
    "#geoCountsAllDF = geoCountsAllDF.sort_values(by='Count')\n",
    "\n",
    "# Assign the x and y axes to specific columns.\n",
    "geoCountsAllDF.plot(x ='CountyName', y='Count', kind = 'bar', figsize=(20, 6))\n",
    "\n",
    "# Show us the resulting bar chart.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can this bar graph tell us? What can it *not* tell us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Map the number of laws per county. <a class=\"anchor\" id=\"5\"></a>\n",
    "\n",
    "To answer that last question--what can the bar graph *not* show us--we can create a map that will show us the number of laws per county geographically. As a reminder, here are the steps we'll need to go through to create this map:\n",
    "\n",
    " - Import geographic county shapes, or polygons stored in a GIS format called a [shapefile](https://en.wikipedia.org/wiki/Shapefile).\n",
    " - Match the county names in our laws-per-county dataframe to the county shapes.\n",
    " - Use matplotlib and geopandas to create a [choropleth map](https://en.wikipedia.org/wiki/Choropleth_map) to show us the number of laws per county in a geographic visualization (map).\n",
    "\n",
    "To **import county shapes**, we'll pull the data we need from the North Carolina Department of Transportation, which [maintains geographic data such as county boundaries for public use](https://connect.ncdot.gov/resources/gis/Pages/GIS-Data-Layers.aspx). We'll use a script similar to the one we used to pull the Jim Crow corpus from the Carolina Digital Repository in the [last module.](05-StructuringOCRData.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requests helps us call up a webpage, or link to a file stored online,\n",
    "# and access the content.\n",
    "import requests\n",
    "\n",
    "# Create a variable to hold the direct link to the county boundaries shapefile.\n",
    "url = 'https://xfer.services.ncdot.gov/gisdot/DistDOTData-Current/State/shpe/CountyBoundary.zip'\n",
    "\n",
    "# Here's where we use the requests module to call up\n",
    "# the content at the url we specified above.\n",
    "r = requests.get(url)\n",
    "\n",
    "# We'll create a new empty .zip file to hold the contents of the shapefile.\n",
    "with open('CountyBoundary.zip', 'wb') as f:\n",
    "    \n",
    "    # Write the contents of the online file into the new file.\n",
    "    f.write(r.content)\n",
    "\n",
    "# When finished, print the following:\n",
    "print('GIS files downloaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click the Jupyter icon at the top of your browser window to access the files accompanying this module. See if htere is now a `CountyBoundary.zip` file located there. You can also click on [county boundaries here](CountyBoundary.zip) to open the file on your local computer. If you do this and extract the .zip file, you should find a folder that looks like the image below: a folder containing a number of different files all with the same file name but with different file extensions. There is one `.shp`, or shapefile, and the other files are basically supporting files--we need all of them in order to access the county boundaries. In Binder, we'll keep the shapefiles compressed in `CountyBoundary.zip` to ensure we do not make any changes to these files.\n",
    "\n",
    "<img src=\"images/10-explore-15.jpg\" width=\"70%\" style=\"margin:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"The uncompressed CountyBoundary.zip file showing the .shp and accompanying files that make up a shapefile.\" title=\"The uncompressed CountyBoundary.zip file showing the .shp and accompanying files that make up a shapefile.\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at these county shapes to see what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll need the following modules to access and view the county boundaries.\n",
    "\n",
    "# Pandas, which we've been working with above, helps us view and organize data.\n",
    "import pandas as pd\n",
    "\n",
    "# Geopandas is an extension of Pandas, which helps us work with geospatial data.\n",
    "import geopandas as gpd\n",
    "\n",
    "# Shapely helps us work with and display geospatial objects. \n",
    "# (Points, lines, polygons.) Geopandas relies on Shapely.\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Geopandas also needs matplotlib to display visualizations.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Libraries successfully imported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's pull in the geographic data.\n",
    "\n",
    "# Open and read the county boundaries shapefile.\n",
    "countyShapes = gpd.read_file('zip://CountyBoundary.zip!CountyBoundary_SHP/BoundaryCountyPolygon.shp')\n",
    "\n",
    "# Display a preview of the county boundaries data.\n",
    "countyShapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have been expecting to see the shapes of county boundaries themselves, and we will see that. But first, it's important to understand that geographic data can also be accessed in a tabular format. *Take a look at all of the columns. What might they represent?*\n",
    "\n",
    "The columns we'll be working with for this exercise are `CountyName` and `geometry`. We can ignore the other columns for now.\n",
    "\n",
    "OK, *now* we'll see what the county shapes, defined in the `geometry` column, actually look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use matplotlib to preview the county boundaries.\n",
    "# \"color\" is the color that will \"fill\" the counties.\n",
    "# \"edgecolor\" is the county boundaries', or outlines', color.\n",
    "countyShapes.plot(color='purple', edgecolor='white', figsize=(10,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our counties, let's **match the county names in our laws-per-county dataframe to the county shapes we just downloaded.** Essentially, we're joining two separate datasets using county names, which should be the same between the two datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create a Pandas dataframe for countyShapes.\n",
    "countyShapesDF = pd.DataFrame(countyShapes)\n",
    "\n",
    "# For each row in countyShapesDF, do the following:\n",
    "for i,row in countyShapesDF.iterrows():\n",
    "    \n",
    "    # Locate the county name in that row's CountyName column.\n",
    "    county = countyShapesDF.loc[i,'CountyName']\n",
    "    \n",
    "    # At the same location as above,\n",
    "    # change the county name to all lowercase. \n",
    "    # This will ensure that the county name in countyShapesDF\n",
    "    # and the county name in geoCountsDF (our number of\n",
    "    # laws-per-county dataframe) match exactly before we try\n",
    "    # to combine dataframes. This process is case sensitive!\n",
    "    countyShapesDF.at[i,'CountyName'] = county.lower()\n",
    "\n",
    "# Now that all CountyNames are lower case,\n",
    "# we'll join geoCountsDF and countyShapesDF.\n",
    "# The below line tells Python to look for the\n",
    "# county names that match in each dataframe\n",
    "# and use those matching names to combine rows.\n",
    "mergedGeoDF = geoCountsAllDF.merge(countyShapesDF, left_on=['CountyName'], right_on=['CountyName'])\n",
    "\n",
    "# Display a preview of the new combined dataframe.\n",
    "mergedGeoDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Examine this new dataframe. What do you notice? How have our two dataframes been combined?* \n",
    "\n",
    "Note that there is only **1** `CountyName` column in the merged data, and the `Count` column from `geoCountsDF` has been added before the other columns from the `countyShapesDF`. We really only need `CountyName`, `Count`, and `geometry` to create a map, but we'll leave these other columns in.\n",
    "\n",
    "**Now we have our final dataset that we can use to map the number of Jim Crow laws per county.** We're going to create a [choropleth map](https://en.wikipedia.org/wiki/Choropleth_map), a map whose colors, or in this case color hues, represent a quantity--in this case, number of laws identified for a given county. A common practice in these kinds of maps is to make darker tones correspond with larger quantities.\n",
    "\n",
    "Let's see how this works with Geopandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to convert our Pandas dataframe\n",
    "# into a Geopandas dataframe. When we preview the\n",
    "# dataframe below, we won't see a difference, but\n",
    "# Python will because it now knows where to find\n",
    "# the geographic coordinates for our county boundaries--\n",
    "# in the geometry column.\n",
    "mergedGeoDF = gpd.GeoDataFrame(mergedGeoDF, geometry='geometry')\n",
    "\n",
    "# Display a preview of our new Geopandas dataframe.\n",
    "mergedGeoDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create our map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://towardsdatascience.com/a-beginners-guide-to-create-a-cloropleth-map-in-python-using-geopandas-and-matplotlib-9cc4175ab630\n",
    "\n",
    "# Set the column that will determine color value\n",
    "# for each county.\n",
    "countVariable = 'Count'\n",
    "\n",
    "# Set a range of color values. Note that\n",
    "# max here is set to match the largest number\n",
    "# of laws per county.\n",
    "vmin, vmax = 0, 26\n",
    "\n",
    "# Create the Matplotlib figure box and axes.\n",
    "# This step lets us include multiple layers of\n",
    "# information inside the same visualization.\n",
    "fig, ax = plt.subplots(1, figsize=(30, 10))\n",
    "\n",
    "# Turn off the axis. We don't need to see\n",
    "# the X and Y axes to read this map.\n",
    "ax.set_axis_off()\n",
    "\n",
    "# Add a title and source information to the map.\n",
    "# If we save this as an image later, we want to know\n",
    "# what we're showing and where your data came from.\n",
    "\n",
    "# Set the title and font size and weight.\n",
    "ax.set_title('Number of Jim Crow Laws per County in North Carolina (1866-1967)', \n",
    "             fontdict={'fontsize': '25', 'fontweight' : '3'})\n",
    "\n",
    "# Add a source citation and set font size. The xy coordinates here \n",
    "# specify where to place the source information in the visualization.\n",
    "ax.annotate('Source: On the Books: Algorithms of Resistance', \n",
    "            xy=(0.6, .05), xycoords='figure fraction', fontsize=12)\n",
    "\n",
    "# Add the sliding scale legend (color bar) and set its colors to\n",
    "# match those of the states.\n",
    "sm = plt.cm.ScalarMappable(cmap='Oranges', norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "fig.colorbar(sm)\n",
    "\n",
    "# Create the map. \n",
    "# Column tells Python where to find our quantitative data\n",
    "# Cmap sets the color scheme: https://matplotlib.org/2.0.2/users/colormaps.html\n",
    "# Linewidth sets the width of the county boundaries.\n",
    "# Ax pulls in the settings above--including turning of the axes and\n",
    "# adding a title and source information.\n",
    "# Edgecolor sets the county boundaries color from between 0 (black)\n",
    "# to 1 (white).\n",
    "mergedGeoDF.plot(column=countVariable, \n",
    "                 cmap='Oranges', \n",
    "                 linewidth=0.8, \n",
    "                 ax=ax, \n",
    "                 edgecolor='0.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Consider our map: what does it tell you? What does it not tell you?*\n",
    "\n",
    "If you're familiar with North Carolina, it might be easy to read this map: you know which counties are where and have some cultural knowledge about the make up of these counties. If you're less familiar with North Carolina, this map may provide limited meaning. In both cases, it may be difficult to compare the legend to the color of each county. \n",
    "\n",
    "To make the map a little more explicit, we can add labels. This process requires that we find the center (centroid) of each county so that we can tell Python where to place each label in relation to its county. The following is the same code as we used above with the addition of several lines to create the labels before producing the final visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://towardsdatascience.com/a-beginners-guide-to-create-a-cloropleth-map-in-python-using-geopandas-and-matplotlib-9cc4175ab630\n",
    "\n",
    "# Set the column that will determine color value\n",
    "# for each county.\n",
    "countVariable = 'Count'\n",
    "\n",
    "# Set a range of color values. Note that\n",
    "# max here is set to match the largest number\n",
    "# of laws per county.\n",
    "vmin, vmax = 0, 26\n",
    "\n",
    "# Create the Matplotlib figure box and axes.\n",
    "# This step lets us include multiple layers of\n",
    "# information inside the same visualization.\n",
    "fig, ax = plt.subplots(1, figsize=(30, 10))\n",
    "\n",
    "# Turn off the axis. We don't need to see\n",
    "# the X and Y axes to read this map.\n",
    "ax.set_axis_off()\n",
    "\n",
    "# Add a title and source information to the map.\n",
    "# If we save this as an image later, we want to know\n",
    "# what we're showing and where your data came from.\n",
    "\n",
    "# Set the title and font size and weight.\n",
    "ax.set_title('Number of Jim Crow Laws per County in North Carolina (1866-1967)', \n",
    "             fontdict={'fontsize': '25', 'fontweight' : '3'})\n",
    "\n",
    "# Add a source citation and set font size. The xy coordinates here \n",
    "# specify where to place the source information in the visualization.\n",
    "ax.annotate('Source: On the Books: Algorithms of Resistance', \n",
    "            xy=(0.6, .05), xycoords='figure fraction', fontsize=12)\n",
    "\n",
    "# Add the sliding scale legend (color bar) and set its colors to\n",
    "# match those of the states.\n",
    "sm = plt.cm.ScalarMappable(cmap='Oranges', norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "fig.colorbar(sm)\n",
    "\n",
    "# Create the map. \n",
    "# Column tells Python where to find our quantitative data\n",
    "# Cmap sets the color scheme: https://matplotlib.org/2.0.2/users/colormaps.html\n",
    "# Linewidth sets the width of the county boundaries.\n",
    "# Ax pulls in the settings above--including turning of the axes and\n",
    "# adding a title and source information.\n",
    "# Edgecolor sets the county boundaries color from between 0 (black)\n",
    "# to 1 (white).\n",
    "mergedGeoDF.plot(column=countVariable, \n",
    "                 cmap='Oranges', \n",
    "                 linewidth=0.8, \n",
    "                 ax=ax, \n",
    "                 edgecolor='0.5')\n",
    "\n",
    "# We want to create a label that includes both the county name and the\n",
    "# number of identified Jim Crow laws. To do this, we need to create a\n",
    "# column in our dataframe that will hold each county's label.\n",
    "\n",
    "# First, we'll create an empty list that will hold the labels.\n",
    "labels = []\n",
    "\n",
    "# Next we'll go through each row to get the county name and count\n",
    "# and create a label.\n",
    "for i,row in mergedGeoDF.iterrows():\n",
    "    \n",
    "    # We'll concatenate (combine) the specific county name and its count\n",
    "    # to create a label. We'll separate these with an escape character \"\\n\",\n",
    "    # which will display the count on a separate line from the county name.\n",
    "    label = str(mergedGeoDF.loc[i, 'CountyName']) + \"\\n\" + str(mergedGeoDF.loc[i, 'Count'])\n",
    "    \n",
    "    # We'll add the label we just made to the labels list.\n",
    "    labels.append(label)\n",
    "\n",
    "# We'll convert the completed list of labels into a new column in mergedGeoDF.\n",
    "mergedGeoDF['Label'] = labels\n",
    "\n",
    "# Now we'll apply those labels to the map. \n",
    "mergedGeoDF.apply(lambda x: ax.annotate(text=x.Label, xy=x.geometry.centroid.coords[0], ha='center', size=12, bbox={'facecolor': 'lightgrey', 'alpha':0.6, 'pad': 1, 'edgecolor':'none'}),axis=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final line that applies labels is complex, so we'll break it down here:\n",
    "\n",
    "`mergedGeoDF.apply(lambda x: ax.annotate(text=x.Label, xy=x.geometry.centroid.coords[0], ha='center', size=12, bbox={'facecolor': 'white', 'alpha':0.6, 'pad': 1, 'edgecolor':'none'}),axis=1);`\n",
    "\n",
    "1. `mergedGeoDF.apply()` - apply everything within the `()` to the plotted geopandas dataframe `mergedGeoDF`.\n",
    "\n",
    "2. `lambda x:` - `lambda` is Python's keyword for an [anonymous function](https://towardsdatascience.com/anonymous-functions-in-python-914a0a4b86ea). Basically, we're asking Python to apply a particular process to every county in our map, and we're using `lambda` to make that happen.\n",
    "\n",
    "3. `ax.annotate()` - This is the expression, or the action, that we want to apply using `lambda` to all counties. This will add a text layer over specific locations (which we'll specify) in our plotted map.\n",
    "\n",
    "4. `text=x.Label, xy=x.geometry.centroid.coords[0], ha='center', size=12, bbox={'facecolor': 'white', 'alpha':0.6, 'pad': 1, 'edgecolor':'none'}),axis=1` - all the parameters that help specify where annotations will go and what they'll look like. We can break these down even further:\n",
    "\n",
    "    - `text=x.Label` - the text contained in the annotation. Remember, we are pulling this from the new `Label` column we created.\n",
    "    \n",
    "    - `xy=x.geometry.centroid.coords[0]` - the xy (cartesian) coordinates we'll use to pin the label above to a specific location within the county boundaries. Here, we'll find the center (centroid) of the county (defined in the `geometry` column).\n",
    "    \n",
    "    - `ha='center'` - `ha` is short for `horizontalalignment` -- the text's alignment in its location. Options besides `center` are `left` or `right`. Change this and rerun the above script to see the change.\n",
    "    \n",
    "    - `size=12` - Font size. Change the number `12` to see the font larger or smaller.\n",
    "    \n",
    "    - `bbox={'facecolor': 'white', 'alpha':0.6, 'pad': 1, 'edgecolor':'none'})` - because we are working with text that appears over both very light and very dark color shades, we want to be sure we can always see the text annotation. We've used `bbox` to add a small box around the text that helps contrast it with its county color. The color (`facecolor`) can be changed as can the opacity (`alpha`). `alpha` needs a value between 0.0 and 1.0 (inclusive). `pad` specifies how large the box is around the text. `edgecolor` gives the box an outline, or as in this case `none`.\n",
    "    \n",
    "    - `axis=1` - a parameter of the `apply()` function, specifying `1` essentially tells Python to apply the entire `lambda` function to the `mergedGeoDF`'s column. You can [read more about this in the Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html).\n",
    "    \n",
    "\n",
    "**Once you've worked through the code to understand how the labels were created, consider how they look.** If you are using this map for your own analysis, what can you learn from it? If you are sharing this map with others, what might need to change? Styling a map such as this, and any of the visualizations we've created in this module, for presentation requires some additional knowledge that you might find the following helpful:\n",
    "\n",
    "- [Design Principles for Cartography (ESRI)](https://www.esri.com/arcgis-blog/products/product/mapping/design-principles-for-cartography/)\n",
    "- [Labeling and text hierarchy in cartography (Axis Maps)](https://www.axismaps.com/guide/labeling)\n",
    "- [From Data to Design (PSU GEOG 486 Cartography and Visualization)](https://www.e-education.psu.edu/geog486/node/517)\n",
    "\n",
    "\n",
    "*Styling in geopandas & matplotlib may require additional learning, and there are other Python libraries that you might look at to assist with this as well, including [Seaborn](https://seaborn.pydata.org/) and [Bokeh](https://bokeh.org/).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <p><strong>As with temporal questions, we might next explore questions that require multiple approaches (temporal, spatial, topical, quantitative).</strong> Here are a few questions we've considered. What kinds of other questions come to mind?</p>\n",
    "        <ul>\n",
    "            <li>Do some parts of the state, including counties, towns, or other place names, appear more frequently than others?</li>\n",
    "            <li>Can possible connections between topics and place names be identified?</li>\n",
    "            <li>Are specific place names distributed throughout the corpus, or are they temporally clustered?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap Up & Next Steps <a class=\"anchor\" id=\"next-steps\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! If you're reading this, then you've likely completed some or all of the *On The Books* tutorials. Well done! \n",
    "\n",
    "**But, you might ask, what now?** There are many possible next steps. You might\n",
    "- Go back through these tutorials using your own corpus of *non-sensitive* text data.\n",
    "    - *If you are using Binder (your browser URL will begin with something like \"https://hub-binder.mybinder\"), then you are storing information temporarily on a Binder server. If you are working with sensitive materials or want to be sure your corpus remains secure and available for continued work, go back to the [first module](00-Introduction-AlgorithmsOfResistance.ipynb) and follow the instructions to set up these modules in a local environment on your computer.*\n",
    "- Use these modules as a starting point to build out your own Jupyter Notebooks. You can edit these modules and their scripts per our license (see the end of this tutorial). We recommend doing this in a version of Jupyter Notebooks stored on your local computer. Go back to the [first module](00-Introduction-AlgorithmsOfResistance.ipynb) to learn how to set this up.\n",
    "- If you're an instructor, you might consider ways you can integrate these modules into your teaching.\n",
    "- If you're a UNC community member and want to learn more Python, text analysis, or other ways of working with text data, sign up for a workshop or consultation with UNC Libraries' [Research Hub](https://library.unc.edu/hub/). If you're at another institution, ask your librarians about how you can learn more text analysis and potentially receive consultative support at your university.\n",
    "- If you want to keep learning on your own, check out tutorials on [*The Programming Historian*](https://programminghistorian.org/), [Constellate](https://docs.tdm-pilot.org/topic/why-learn-text-analysis/), or [Hathi Trust](https://teach.htrc.illinois.edu/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources<a class=\"anchor\" id=\"resources\"></a>\n",
    "\n",
    "- Steven Bird, Ewan Klein, and Edward Loper, <a href=\"http://www.nltk.org/book/\" target=\"blank\"><em>Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit</em></a>.\n",
    "\n",
    "- Allan Brown and Bryan Tor, <a href=\"https://guides.library.ucsc.edu/DS/Resources/Voyant#s-lg-box-wrapper-29088760\" target=\"blank\">\"Voyant Tools Tutorial,\"</a> *UC Santa Cruz University Library,* Spring 2019.\n",
    "\n",
    "- Heather Froehlich, <a href=\"https://programminghistorian.org/en/lessons/corpus-analysis-with-antconc\" target=\"blank\">\"Corpus Analysis with AntConc,\"</a> *The Programming Historian.*\n",
    "\n",
    "- Charlie Harper, <a href=\"https://programminghistorian.org/en/lessons/visualizing-with-bokeh#the-basics-of-bokeh\" target=\"blank\">\"Visualizing Data with Bokeh and Pandas,\"</a> *The Programming Historian.*\n",
    "\n",
    "- Hathi Trust Research Center's <a href=\"https://teach.htrc.illinois.edu/modules/\" target=\"blank\">Text Analysis Modules</a>.\n",
    "\n",
    "- Folgert Karsdorp, <a href=\"https://nbviewer.jupyter.org/github/fbkarsdorp/python-course/blob/master/Chapter%203%20-%20Text%20analysis.ipynb\" target=\"blank\">\"Chapter 3: Text Analysis,\"</a> <a href=\"https://www.karsdorp.io/python-course/\" target=\"blank\"><em>Programming with Python for the Humanities.</em></a>\n",
    "\n",
    "- Eric Monson, [\"Spatial Analysis in Python\"](https://github.com/emonson/spatial_analysis/).\n",
    "\n",
    "- Richard A. Paschal, <a href=\"https://cap-press.com/books/isbn/9781531017712/Jim-Crow-in-North-Carolina\" target=\"blank\"><em>Jim Crow in North Carolina: The Legislative Program from 1865 to 1920,</em></a> Durham, NC: Carolina Academic Press, 2021. \n",
    "\n",
    "- Stéfan Sinclair and Geoffrey Rockwell, <a href=\"http://hermeneuti.ca/\" target=\"blank\"><em>Hermeneutica.</em></a>\n",
    "    \n",
    "- William J. Turkel and Adam Crymble, <a href=\"https://programminghistorian.org/en/lessons/counting-frequencies\" target=\"blank\">\"Counting Word Frequencies with Python\",</a> *The Programming Historian.*\n",
    "\n",
    "- <a href=\"https://docs.tdm-pilot.org/topic/why-learn-text-analysis/\" target=\"blank\">\"Why Learn Text Analysis?\"</a> *Constellate.* See also Constellate's <a href=\"https://docs.tdm-pilot.org/topic/intermediate-lessons/\" target=\"blank\">\"Intermediate Lessons.\"</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This module is licensed under the [GNU General Public License v3.0](https://github.com/UNC-Libraries-data/OnTheBooks/blob/master/LICENSE). Individual images and data files associated with this module may be subject to a different license. If so, we indicate this in the module text.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
