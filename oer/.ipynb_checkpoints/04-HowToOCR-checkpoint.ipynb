{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How To OCR with Python & Tesseract: The Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<< Previous module: [What is OCR?](03-WhatIsOCR.ipynb) <<**\n",
    "\n",
    "*1-2 hours*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <strong>Learning Objectives:</strong>\n",
    "    <p>By the end of this module, you should be able to</p>\n",
    "    <ul>\n",
    "        <li>describe the necessary input formats for OCR;</li>\n",
    "        <li>explain the importance of performing adjustments (pre-processing) to inputs before running OCR;</li>\n",
    "        <li>identify adjustments that will likely improve text inputs;</li>\n",
    "        <li>perform pre-processing steps using Python;</li>\n",
    "        <li>describe and implement basic OCR steps using Python and Tesseract in Jupyter Notebooks.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Quick Review: OCR Inputs & Outputs](#quick-review)\n",
    "- [The OCR Process](#ocr-process)\n",
    "- [Preparing Texts for OCR (Pre-Processing)](#pre-processing)\n",
    "- [Performing OCR](#performing-ocr)\n",
    "- [Resources](#resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Review: OCR Inputs & Outputs <a class=\"anchor\" id=\"quick-review\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already discussed this in the [last module](03-WhatIsOCR.ipynb), but it bears repeating. In order to perform OCR on a text corpus, we need the following:\n",
    "\n",
    "- A **single file folder** containing all of the corpus files. If the corpus is small enough (e.g. 1 book), this could be simply a single file.\n",
    "- All corpus files should be of the **same file format**.\n",
    "- The chosen file format should be **interoperable** (usable by many software and operating systems) and stable (changes rarely if ever).\n",
    "\n",
    "\n",
    "- For our work with Python and Tesseract, the files should be **images**, which means that each file will correspond to 1 single-sided page (if in a book format) in the corpus. \n",
    "\n",
    "To keep all of these image files organized, we recommend creating a file structure that looks like the below: 1 file folder for the entire corpus, and 1 subfolder for each volume in the corpus containing an image file for every page in the volume.\n",
    "\n",
    "<img src=\"images/08-ocr-01.jpeg\" width=\"70%\" style=\"padding-top:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Screenshot of a file structure for image files to be OCR'ed.\" title=\"Screenshot of a file structure for image files to be OCR'ed.\" />\n",
    "\n",
    "Note that the file naming structure identifies *both* which volume the images are part of *and* which scanned page they correspond to, which helps us maintain the order of the volume. These numbers *may not* correspond to page numbers because the scanning included outer and inner covers as well as title pages, etc.\n",
    "\n",
    "Note that we are working with .jpg files here. These are files that we [downloaded from the Internet Archive](02-GatheringACorpus.ipynb#how-to-download) and with which any computer should be able to work. The process we'll be using, though, can also be run with .png, .tiff, .jp2, and other common interoperable image formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what we'll produce through the OCR process in this module: **1 file folder containing 1 file per volume in the .txt (plain text) format.** The plain text format is interoperable, stable, and fully computer readable, meaning it will be ready for performing computational analysis in whatever tools you might choose to work with. We'll demonstrate some analysis tools in Python in the [final module in this series](06-ExploratoryAnalysis.ipynb).\n",
    "\n",
    "<img src=\"images/08-ocr-02.jpeg\" width=\"70%\" style=\"padding-top:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Screenshot of the file structure for files after OCR.\" title=\"Screenshot of the file structure for files after OCR.\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The OCR Process<a class=\"anchor\" id=\"ocr-process\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/noun_arrow with loops_2073885.png\" width=\"40%\" style=\"float:right;\" alt=\"arrow with loops by Kalinin Ilya from the Noun Project\" title=\"arrow with loops by Kalinin Ilya from the Noun Project\" />\n",
    "\n",
    "**Producing OCR'ed text is an iterative, rather than a linear, process.** To get the best possible output involves multiple steps and in some instances repetition of steps. Here's an overview of what these steps can look like. \n",
    "\n",
    "*Keep in mind that this process can vary not only based on the complexity and legibility of your corpus but also based on the resources you have. Consider your expertise, whether you are working with a team or by yourself, how much time you have, and your ultimate research goal. These should all factor in to how complex you make your OCR process.*\n",
    "\n",
    "### Pre-Processing:\n",
    "This phase is all about testing to figure out the best adjustments and OCR settings for your corpus:\n",
    "\n",
    "1. **Create a folder of sample text from your corpus.** The size of the sample may depend on the corpus' size and homogeneity or heterogeneity, but it should be an amount that you and/or your team could review manually in a reasonably short period of time.\n",
    "2. **Run OCR on your sample.**\n",
    "3. **Review the output** to identify errors, **looking especially for error *patterns*** that could be addressed at a corpus level.\n",
    "4. **Create a list of errors and [possible adjustments](#pre-processing)** that you might use to address the errors. **Order the list based on which errors should be solved first--which might address the largest number of errors.** For example, it would be more important to fix rotated or skewed pages across the sample/corpus before trying to use erosion or dilation to make specific pages more legible to Tesseract. \n",
    "5. **Make the first adjustment** on your list to the sample.\n",
    "6. **Re-run OCR on your sample.**\n",
    "7. **Review the output.** Has the output improved noticeably? Are there still errors and error patterns? \n",
    "8. **Repeat some or all of the above steps:** Depending on your findings, you might continue applying adjustments from your list, re-running OCR, and reviewing outputs, or you might be ready to move on to the next step. \n",
    "\n",
    "Depending on the *complexity* of your corpus, you may want to select a few different samples to complete this process with and then compare the adjustments you make across all samples to see which adjustments work best overall, and which might *introduce* errors to certain parts of the corpus. \n",
    "\n",
    "If need be, you could consider running OCR on separate parts of your corpus. The *On The Books* team did this because marginalia (text printed in the margins) appear through a significant portion of the corpus but are no longer printed after a certain point. Removing marginalia wasn't necessary once it stopped appearing in the text, so that step could be skipped for the later portion of texts.\n",
    "\n",
    "### Performing OCR:\n",
    "Once you're satisfied with the pre-processing on your sample(s), here's where you run the actual OCR. This part of the process may also be iterative:\n",
    "\n",
    "1. **Apply your chosen pre-processing adjustments** to your entire corpus.\n",
    "2. **Run OCR** on your entire corpus.\n",
    "3. **Pull samples from your output to review.** Do you notice any recurring or new errors? If so, you may need to return to pre-processing to assess and address these errors.\n",
    "4. **Repeat steps 1-3 as needed.** If you have a very large corpus, you may consider running these steps in batches and iterating through each batch.\n",
    "\n",
    "### \"Cleaning\" OCR:\n",
    "**This part of the process is often best performed with a combination of manual (human) and automated (computer) steps.** This is where you may be addressing not only errors in the OCR itself but also issues with the original printing, as we describe below with regard to [hyphenated words at the end of lines](#hyphens). As with pre-processing, how complex you make iterations in this phase depends on your corpus and your resources:\n",
    "\n",
    "1. **Use Python to check for and correct possible spelling errors.** This step should focus on common words and avoid proper nouns. As with any automated step, it's possible that new errors will be introduced here. \n",
    "\n",
    "    1. If there is a known and small quantity of proper nouns used in individual texts or across the corpus, and these are consistently \"read\" incorrectly by Tesseract, it may be possible to use Python to correct these.\n",
    "    \n",
    "2. If your corpus is small enough and/or you have a team that can help you, **read through the corpus** to manually check for and correct errors. This may be a moment to correct proper nouns. If you have a team, it may be advisable to have texts read and corrected by multiple team members. It will be important that these team members have access to both inputs and outputs, and perhaps even lists of proper nouns, to be able to compare the original scans with the computer-readable versions. You may even want to set up a process whereby reviewers can flag words they are not sure about so that another reviewer can provide their opinion so that you and/or another project manager making a final decision on uncertain words.\n",
    "\n",
    "The above process could be broken down further to address smaller issues incrementally and iteratively. It may also be useful to break your corpus into units of analysis before or during this process to assist with cleaning. We will discuss the splitting process when we structure our OCR'ed data in the [next module](05-StructuringOCRData.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Texts for OCR (Pre-Processing)<a class=\"anchor\" id=\"pre-processing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember all of the questions we considered in the [last module](03-WhatIsOCR.ipynb)? The ones that asked us to think about text format (print/handwritten), format and orientation on the page, legibility, and so on? In this section, we'll walk through the steps that need to be done to address especially **format and orientation on a page and legibility** before performing OCR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which files?\n",
    "\n",
    "For this tutorial, we've given you only a sample of pages to work with from the 1955 NC session laws, beginning with the first chapter (page 1). You may have noticed that the file name ends with `0057` rather than `0001`. This is because scanned session law volumes often include other content, such as a scan of the outer covers, a table of contents, and the North Carolina state constitution, appearing before or after the laws themselves. Because the *On The Books Team* is only interested in the laws themselves, we went through each volume and removed all images that we did't need. The lesson here: **don't run OCR on content that you don't need.** \n",
    "\n",
    "<img src=\"images/08-ocr-03.jpeg\" width=\"70%\" style=\"padding-top:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Screenshot of the filenames for the 1955 volume, showing that the first page of laws is the file ending in 0057.\" title=\"Screenshot of the filenames for the 1955 volume, showing that the first page of laws is the file ending in 0057.\" />\n",
    "\n",
    "When you are preparing a corpus for OCR, in addition to becoming familiar with potential issues you see on each page, remove all files that don't need to be OCR'ed. *If you want to keep a copy of the complete un-OCR'ed corpus, make a copy of it **before** deleting image files from the version that you will use to perform OCR.* If you find that there is a consistent number of pages that you want to remove at the beginning and/or end of each corpus, it's possible to use Python to do this. Use caution if you choose this route, though, as it can be hard to undo deletions performed programmatically. At the end of the day *you probably know better than your computer* which files you need and which you don't.\n",
    "\n",
    "#### Missing pages? Duplicate pages?\n",
    "\n",
    "Something else to note about the 1955 laws in particular: The first two files in the scanned volume, ending `0000` and `0001`, happen to be scans of pages in the middle of the volume. These could be included at the beginning for a few reasons: \n",
    "- the archivists performing the scan accidentally skipped these pages during the initial scan; or\n",
    "- the initial scan of these pages was in some way inadequate (perhaps the page was blurred, or the scanner malfunctioned).\n",
    "\n",
    "<img src=\"images/08-ocr-04.jpeg\" width=\"70%\" style=\"padding-top:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Screenshot of first files in the 1955 volume, which are pages scanned from the middle of the volume.\" title=\"Screenshot of first files in the 1955 volume, which are pages scanned from the middle of the volume.\" />\n",
    "\n",
    "Regardless of the reason, now is the right moment to **check whether these pages were indeed skipped** and make a note of this so that you can be sure they are included in the OCR and final dataset. As it happens, these pages were *not* skipped but were duplicates, so we would remove them. If you *don't* find pages like these out of order in your own corpus, either indicated by page numbers in the images themselves or by numbered file names skipping an integer, *it may not be worth your time to check every page* (particularly if you are dealing with thousands of pages). It's possible to identify skipped pages in the data structuring process and add them to the larger dataset. \n",
    "\n",
    "#### A Note on Cropping\n",
    "\n",
    "Our duplicate pages do also point out something else important: **whether you need to crop scanned pages.**\n",
    "\n",
    "<img src=\"images/08-ocr-05.jpeg\" width=\"70%\" style=\"padding-top:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Screenshot of the duplicate images of page 592, the left image not cropped and the right image cropped.\" title=\"Screenshot of the duplicate images of page 592, the left image not cropped and the right image cropped.\" />\n",
    "\n",
    "When documents are scanned, often there is more included in the image than just the document itself: the stand or supports for the document, color calibration targets, rulers, and anything else in close proximity to the document.  Archivists preparing scanned materials for the Internet Archive and other digital repositories may crop out all parts of a scanned image that are *not* part of the document, aiming to create image files of a relatively uniform size.\n",
    "\n",
    "If your images have not been cropped already, **here are a few resources for learning how to batch crop images:**\n",
    "- In Python: [this Jupyter Notebook explains how to prepare to crop](https://github.com/UNC-Libraries-data/OnTheBooks/blob/master/examples/marginalia_determination/marginalia_determination.ipynb), and [this Notebook implements the crop along with other adjustments we'll explore further here](https://github.com/UNC-Libraries-data/OnTheBooks/blob/master/examples/adjustment_recommendation/adjRec.ipynb)\n",
    "- [In Photoshop](https://www.linkedin.com/learning-login/share?forceAccount=false&redirect=https%3A%2F%2Fwww.linkedin.com%2Flearning%2Flearning-photoshop-automation%3Ftrk%3Dshare_ent_url%26shareId%3D9wq0fJRcSEOBjgywFT9gOA%253D%253D&account=42563596) (UNC or LinkedIn Learning log in required)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <p><strong>If you're doing the scanning yourself or will be working with someone to newly digitize materials,</strong> it's a good idea to carefully plan your scanning process. Every step matters in terms of generating the best possible OCR results. Digital NC have posted their <a href=\"https://www.digitalnc.org/policies/digitization-guidelines/\" alt=\"Digital NC digitization guidelines\">digitization guidelines</a> along with <a href=\"https://www.digitalnc.org/about/what-we-use-to-digitize-materials/\" alt=\"Digital NC scanning equipment\">descriptions of their scanning equipment</a>. These can provide a helpful starting point if you will be beginning your project with undigitized materials.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a closer look at the text we'll use to practice performing OCR:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <p>Drawing from <a href=\"https://onthebooks.lib.unc.edu/laws/all-laws/\" alt=\"On the Books corpus\"><em>On The Books</em> corpus</a>, we'll be working with the North Carolina session laws from 1955: <a href=\"https://archive.org/details/sessionlawsresol1955nort/\" alt=\"1955 NC session laws on the Internet Archive\">https://archive.org/details/sessionlawsresol1955nort/</a>.</p>\n",
    "    <p>Open the 1995 volume in the Internet Archive and skim through, considering the following questions:</p>\n",
    "    <ul>\n",
    "        <li>How is the text formatted on the page?</li>\n",
    "        <li>How is the text oriented on the page?</li>\n",
    "        <li>Do you notice any pages or sections that might cause an error in the OCR? What are they? What kind of error do you think they might cause?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few possible considerations with this volume that we'll want to address with pre-processing:\n",
    "- broken words\n",
    "- possible noise (e.g. the visible shadow of text from other pages)\n",
    "- slanted, or skewed, text\n",
    "- crooked, or rotated, text\n",
    "- occasional pencil marks on the text\n",
    "\n",
    "Did you notice any other possible issues? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand why we're going to spend significant time pre-processing, let's look at the very basic OCR process with pyTesseract.\n",
    "\n",
    "First, we need to run the following line of code to finish installing Tesseract. **Wait until the following code finishes (it will take 1-2 minutes), and you don't see a star by the code or hourglass in your browser tab before you continue.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tesseract on Binder.\n",
    "# The exclamation runs the command as a terminal command.\n",
    "# This may take 1-2 minutes.\n",
    "# Source: Nathan Kelber & JStor Labs Constellate team.\n",
    "%conda install -c conda-forge -y tesseract\n",
    "%conda install -c conda-forge pytesseract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Has the above code finished running? If so, you can now run this code to OCR 1 page of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Image module from the Pillow Library, which will help us access the image.\n",
    "from PIL import Image\n",
    "\n",
    "# Import the pytesseract library, which will run the OCR process.\n",
    "import pytesseract\n",
    "\n",
    "# Open a specific image file, convert the text in the image to computer-readable text (OCR),\n",
    "# and then print the results for us to see here.\n",
    "print(pytesseract.image_to_string(Image.open(\"sessionlawsresol1955nort_0057.jpg\"), lang=\"eng\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty simple, right? If you haven't run it yet, go ahead. \n",
    "\n",
    "We used [this image file](sample/sessionlawsresol1955nort_0057.jpg) to test out the code. Open it up and compare it to the text we just printed. What do you notice about the layout, text format, and characters in the image and text versions?\n",
    "\n",
    "<img src=\"images/sessionlawsresol1955nort_0057.jpg\" width=\"30%\" style=\"padding-top:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"A test OCR page; the first page of the 1955 North Carolina session laws.\" title=\"A test OCR page; the first page of the 1955 North Carolina session laws.\" />\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the above looks promising--and we could go ahead and create a loop to run this code on every image file in the 1955 volume--we always run the risk of introducing errors into our plain text output that can be avoided by some pre-processing steps. So let's pause on the OCR for a bit and look at some of the steps you might need to take into consideration with your own materials. Although Tesseract does an overall good job addressing these issues when they are minor, it may be worth your while to fix any issues you notice *before* running Tesseract to avoid introducing errors into OCR'ed text in the first place:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rescaling\n",
    "**The higher quality the digitization, the better the OCR**--this is the general rule. \"Quality\" has a lot to do with the OCR requirements we've already covered as well as those we'll cover below. We can begin, though, with the number of pixels per image--that is, the number of pixels per *inch*. Remember that computers present images as a grid of pixels, usually squares but sometimes rectangles, and that each carry specific color information. Put hundreds, thousands, millions of pixels together, and we have an image. \n",
    "\n",
    "<img src=\"images/07-ocr-01.jpeg\" width=\"70%\" style=\"padding-top:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Screenshot of text stored in an image format from a page of North Carolina laws\" title=\"Screenshot of text stored in an image format from a page of North Carolina laws\" />\n",
    "\n",
    "A common way for computer programmers to measure image quality is by assessing the number of pixels per inch (ppi). This is important for many reasons: a photographer will want to keep their number of pixels high (perhaps 300 ppi) in preparation for printing, but a web designer will want a much lower number of pixels (72 ppi) to keep an image looking crisp while also keeping file sizes small to avoid slowing down webpage loading time. If you've ever opened a webpage and seen text but had to wait a few seconds for images to load, you've seen the difference between how long it takes for text vs. an image to load. The more pixels, the larger the file (in kilobytes, megabytes, or even gigabytes), and large files take longer to move from a server to your computer--add in low bandwidth internet, and the load time increases exponentially. \n",
    "\n",
    "So, what's the difference? Let's look:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"row\" style=\"padding-bottom:20px;\">\n",
    "    <div class=\"column\">\n",
    "<img src=\"images/08-ocr-06.jpeg\" width=\"40%\" style=\"float:left; padding-top:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"An image of the letter S at 72 ppi.\" title=\"An image of the letter S at 72 ppi.\" />\n",
    "    </div>\n",
    "    <div class=\"column\">  \n",
    "<img src=\"images/08-ocr-07.jpeg\" width=\"39%\" style=\"float:right; padding-top:20px; margin-right:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"An image of the letter S at 300 ppi.\" title=\"An image of the letter S at 300 ppi.\" />\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "The image on the left shows a scanned letter S at 72 ppi. The visible squares represent individual pixels. Note that each pixel represents one color from the page, and there is a transition between pixels representing ink and those representing paper. \n",
    "\n",
    "The image on the right is the same letter S rescaled to 300 ppi. The squares here appear smaller because there are far more of them. Note that instead of there being only a line 1-2 pixels wide making up the S shape, there are far more--far more for Tesseract to \"read\" and interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <p><a href=\"https://tesseract-ocr.github.io/tessdoc/ImproveQuality\" target=\"blank\">Per its documentation</a>, Tesseract works best with an image resolution of 300 ppi. The documentation actually uses \"dpi\", or <a href=\"https://en.wikipedia.org/wiki/Dots_per_inch\" target=\"blank\">\"dots per inch\"</a>. If you're beginning your project by scanning materials, this unit will be important when you set up your scanner, but once you move into image processing, we're dealing with <a href=\"https://en.wikipedia.org/wiki/Pixel_density\" target=\"blank\">pixels per inch</a>. These are not the same, but many people use dpi and ppi interchangeably.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images we downloaded from the Internet Archive are 72ppi--optimized for web viewing. In an ideal world, we'd go back and use the .jp2 files, which are higher resolution. While it's <mark style=\"background-color:lightblue;\">possible</mark> to rescale (increase the ppi) of our sample images, increasing ppi involves using algorithms that can change, in minute ways, an image file. Those minute changes could make a larger impact on OCR results depending on the original image. For this reason, **it's always better to start by creating high resolution image files in the scanning process.** For our purposes here, PyTesseract has already proven that it's working well with the .jpg 72 ppi versions of the Internet Archive files. If you're interested in learning more about rescaling images, though, try this [GeeksForGeeks tutorial](https://www.geeksforgeeks.org/python-pil-image-resize-method/) as a starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rotating & Deskewing\n",
    "Sometimes, in spite of everyone's best efforts, a document is scanned at a slight angle, either rotated slightly on the scan bed or perhaps photographed at a slight angle, introducing a skew. The result can look like these images:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"row\"  style=\"padding-bottom:20px;\">\n",
    "    <div class=\"column\">\n",
    "<img src=\"images/sessionlawsresol1955nort_0057_rotated.jpg\" width=\"40%\" style=\"float:left; padding-top:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"A test OCR page that has been rotated; the first page of the 1955 North Carolina session laws.\" title=\"A test OCR page that has been rotated; the first page of the 1955 North Carolina session laws.\" />\n",
    "    </div>\n",
    "    <div class=\"column\">  \n",
    "<img src=\"images/sessionlawsresol1955nort_0057_skewed.jpg\" width=\"37%\" style=\"padding-left:10px; padding-top:20px; float:right; margin-right: 40px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"A test OCR page that has been skewed; the first page of the 1955 North Carolina session laws.\" title=\"A test OCR page that has been skewed; the first page of the 1955 North Carolina session laws.\" />\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Left:** A rotated version of our sample page. **Right:** A skewed version of our sample page.\n",
    "\n",
    "Let's run these through PyTesseract to see how it handles them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCR the rotated page.\n",
    "print(pytesseract.image_to_string(Image.open('images/sessionlawsresol1955nort_0057_rotated.jpg')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the OCR results for the rotated page compare to our attempt with the original (unrotated) file? \n",
    "\n",
    "Now let's try the skewed version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCR the skewed page.\n",
    "print(pytesseract.image_to_string(Image.open('images/sessionlawsresol1955nort_0057_skewed.jpg')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at this output carefully. The skewed page might appear to be largely similar to the original (non-skewed) page we tried first. Where are there errors, though? What kinds of problems might these errors cause for later analysis?\n",
    "\n",
    "**Why do errors occur when reading a rotated or skewed text?** Tesseract has been programmed to expect to \"read\" a language in the same way a human would. We read English left to right and from the top of a page down. Although we are able to parse text even when viewing it at an angle (maybe you can even read text upside down), Tesseract doesn't do this well. It will still attempt to read a rotated line from left to right--it won't know to follow the text as it slants down or up. So it returns its interpretation of the letters that fall within its line of \"sight.\" This is why, particularly with rotated texts, we may receive symbols and other unexpected characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Noise\n",
    "Images can't produce sound, but they can still have *noise*. In an image, noise is a **random variation in brightness or color**. Let's look again at our S from earlier:\n",
    "\n",
    "<img src=\"images/08-ocr-06.jpeg\" width=\"51%\" style=\"padding-top:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"An image of the letter S at 72 ppi.\" title=\"An image of the letter S at 72 ppi.\" />\n",
    "\n",
    "The pixels surrounding the S represent the color of the paper the S was printed on. The pixels are not all one color, though. That variation is noise. In these images, the noise has already been minimized in the scanning process: if you open one of the images and zoom in, you may notice that blank page space surrounding text appears to have many pixels that are close to the same color. \n",
    "\n",
    "**Tesseract removes noise on its own, but this process can also introduce errors in images that have a high amount of noise.** If you want to learn more about noise and removing it using Python [here's a good place to start](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_photo/py_non_local_means/py_non_local_means.html). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inverting & Binarizing\n",
    "Early OCR programs required light text on dark backgrounds to operate correctly. In recent years, many OCR programs have moved to preferring dark text on light backgrounds. This means that **inversion** is typically not an issue historians need to worry about since most printed documents are dark text on light background. There might be some exceptions to this if you are working with, for example, images of microfiche. \n",
    "\n",
    "Just for fun, let's see [how Python handles image inversion](https://pillow.readthedocs.io/en/latest/reference/ImageOps.html#PIL.ImageOps.invert) (and you can use this if you ever do need to OCR microfiche):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the modules we need from the PIL library.\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "\n",
    "# Open the original image file.\n",
    "file = Image.open(\"sessionlawsresol1955nort_0057.jpg\")\n",
    "\n",
    "# Use the ImageOps.invert function to invert the colors in the original file.\n",
    "inverted_file = ImageOps.invert(file)\n",
    "\n",
    "# Save the newly inverted image file.\n",
    "inverted_file.save(\"sessionlawsresol1955nort_0057_inverted.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll find the result saved in the [same folder as this Notebook](sessionlawsresol1955nort_0057_inverted.jpg), or you can preview it below:\n",
    "\n",
    "<img src=\"images/sessionlawsresol1955nort_0057_inverted.jpg\" width=\"40%\" style=\"padding-top:20px; margin-bottom:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Inverted version of page 57 from the 1955 North Carolina Session Laws.\" title=\"Inverted version of page 57 from the 1955 North Carolina Session Laws.\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where inversion switches the colors in an image file **binarization converts an image so that it shows image data in only two pixel \"colors\": black and white**. Tesseract does this as part of its OCR process, but it might be worthwhile to do this ahead of time if you're trying to reduce noise or see where, for example, a shadow on a page may introduce problems for Tesseract. A step in that direction that can help is converting images to grayscale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the modules we need from the PIL library.\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "\n",
    "# Open the original image file.\n",
    "file = Image.open(\"sessionlawsresol1955nort_0057.jpg\")\n",
    "\n",
    "# Use the ImageOps.invert function to invert the colors in the original file.\n",
    "inverted_file = ImageOps.grayscale(file)\n",
    "\n",
    "# Save the newly inverted image file.\n",
    "inverted_file.save(\"sessionlawsresol1955nort_0057_grayscale.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result ([again saved with this Notebook](sessionlawsresol1955nort_0057_grayscale.jpg)) in our case is less exciting because our image already had dark ink on a light background. Nonetheless, it helps us see more clearly where there might be variations in background color that will interfere with OCR. In this case, a smudge in the top right corner, far from the text, has become much more visible, as has the shadow near the gutter (where the page meets the volume's binding). Gutter shadows can cause problems if there is little margin between the gutter and page text. Tesseract has an [example of a page](https://tesseract-ocr.github.io/tessdoc/ImproveQuality#binarisation) that demonstrates much more dramatically how binarization can reveal shadows on a page.\n",
    "\n",
    "<img src=\"images/sessionlawsresol1955nort_0057_grayscale.jpg\" width=\"40%\" style=\"padding-top:20px; margin-bottom:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Grayscale version of page 57 from the 1955 North Carolina Session Laws.\" title=\"Grayscale version of page 57 from the 1955 North Carolina Session Laws.\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dilation and erosion\n",
    "Finally, there is dilation and erosion. As we can see in our sample page, printers often varied font thickness when they set the text type. Bold might be used for headings while thinner fonts might be used for smaller text. Depending on the print quality, bolded text might have additional ink around it, while thinner text might not have enough ink. Variation in ink thickness can throw Tesseract off, so **eroding bolded text** (making it thinner) and **dilating very thin text** (adding thickness) can help address this issue.\n",
    "\n",
    "Performing erosion and dilation in Python requires some additional understanding of image processing. We won't cover it here (and our samples don't need it!), but [this GeeksForGeeks tutorial](https://www.geeksforgeeks.org/erosion-dilation-images-using-opencv-python/) explains the basics and provides sample code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying Layout & Text Order\n",
    "\n",
    "There are many instances when we might be working with printed documents that have text arranged in a variety of ways--not just in a single column or orientation on the page.\n",
    "\n",
    "While Tesseract does have tools for estimating a document's orientation, on its own it is not wel equipped to identify text order, recognize images, or understand arrangement of text and images on a page--tasks that many refer to as [\"document layout analysis\"](https://en.wikipedia.org/wiki/Document_layout_analysis) or \"page layout analysis.\" These analysese need to be performed before running Tesseract in order to proide it with th ecorrect ordering and layout information--or, rather, in order to focus its attention on specific parts of a document in a specific sequence. Here is an overview of that workflow:\n",
    "\n",
    "1. Identify the areas on a page that you want Tesseract to focus on. It may be that you want to include only *some* parts of a page and not others. Consider whether this area might be similar or different on different documents.\n",
    "\n",
    "<img src=\"images/chronam_daybook_19151112_pellagra_full.jpg\" width=\"40%\" style=\"padding-top:20px; margin-bottom:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Newspaper page showing text in two columns with a vertical line separating them and an image in the middle of the right column. Source: Chronicling America\" title=\"Newspaper page showing text in two columns with a vertical line separating them and an image in the middle of the right column. Source: Chronicling America\" />\n",
    "\n",
    "\n",
    "2. For each page, calculate the area that you want to *include* in the OCR. To do this, use pixels as cartesian/XY coordinates to mark out an area's corners. The outlines created by these coordinates are referred to as \"bounding boxes\" and may include as much or as little text as needed. This may be automated in a variety of ways using Python but may need some human intervention.\n",
    "\n",
    "<img src=\"images/chronam_daybook_19151112_pellagra_full_bboxes.png\" width=\"40%\" style=\"padding-top:20px; margin-bottom:20px; box-shadow: 25px 25px 20px -30px rgba(0, 0, 0);\" alt=\"Newspaper page showing text in two columns with a vertical line separating them and an image in the middle of the right column. Each column is annotated with a pink bounding box to indicate specific, separate areas of text. Source: Chronicling America\" title=\"Newspaper page showing text in two columns with a vertical line separating them and an image in the middle of the right column. Each column is annotated with a pink bounding box to indicate specific, separate areas of text. Source: Chronicling America\" />\n",
    "\n",
    "\n",
    "3. Create a dataset of all of the bounding boxes on each page. To do this, you may need to specify particular features about the document, such as whether columns are separated by a vertical line or blank space.\n",
    "\n",
    "\n",
    "4. Feed these bounding boxes and the content within them in their \"reading\" order to Tesseract for OCRing.\n",
    "\n",
    "Here is [an example of how *On The Books* did this to exclude marginalia from its OCR](https://github.com/UNC-Libraries-data/OnTheBooks/blob/master/examples/marginalia_determination/marginalia_determination.ipynb).\n",
    "\n",
    "There ar a variety of tools you might use to do this. *On The Books* used [Pillow](https://pypi.org/project/Pillow/) and [NumPy](https://numpy.org/) Python libraries. [OpenCV](https://opencv.org/)'s computer vision tools can also be used for this as can tools such as [Kraken](http://kraken.re/) and [OCRopus](https://ocropus.github.io/). The [Coursera course](https://www.coursera.org/learn/python-project) demonstrates how to do this with OpenCV and Kraken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing OCR<a class=\"anchor\" id=\"performing-ocr\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an understanding of pre-processing steps and their role in reducing OCR errors, let's return to our original sample to get a sense of where we might need to make adjustments. This time, we'll run the code on our full ten-page sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTesseract and PIL, an image processing library used by PyTesseract, to complete the OCR.\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "# Import os, a module for file management.\n",
    "import os\n",
    "\n",
    "# Import re, a module that we can use to search text.\n",
    "import re\n",
    "\n",
    "# Import glob, a module that helps with file management.\n",
    "import glob\n",
    "\n",
    "# Open the file folder where our sample pages are stored.\n",
    "# Look only for the files ending with the \".jpg\" file extension.\n",
    "sampleFilePath = glob.glob(\"sample/*.jpg\")\n",
    "\n",
    "# Create a folder for the volume in the output directory (/sample).\n",
    "outDir = \"sample_output\"\n",
    "newDir = os.path.normpath(outDir)\n",
    "\n",
    "# If you're running this script a second or third time, the sample_output folder will already exist. \n",
    "# The following statement checks whether it already exists and then creates the\n",
    "# sample_output folder if it doesn't exist (e.g. if the statement below is False).\n",
    "if os.path.exists(newDir) == False:\n",
    "    os.mkdir(newDir)\n",
    "\n",
    "# Adding a \"/\" after newDir (\"sample_output\") makes it into a file path that\n",
    "# we'll use to move our output file to the correct folder later in this script.\n",
    "newDir = newDir + \"/\"\n",
    "    \n",
    "# For each file in the sample folder:\n",
    "for file in sampleFilePath:\n",
    "    \n",
    "    # Open a file.\n",
    "    with open(file, 'rb') as inputFile:\n",
    "        \n",
    "        # Read the file using PIL's Image module.\n",
    "        img = Image.open(inputFile)\n",
    "    \n",
    "        # Run OCR on the open file.\n",
    "        ocrText = pytesseract.image_to_string(img)\n",
    "        \n",
    "        # Get a file name -- without the extension -- to use when we name the output file.\n",
    "        fileName = file.strip('.jpg')\n",
    "        \n",
    "        # The current file name also includes its folder name (sampleFilePath, \"sample/\").\n",
    "        # We want to store our text output files in a different folder so that we can use \n",
    "        # them in future without altering the original image files. The following two \n",
    "        # lines use the re module to rename the path from \"sample/\" to \"sample_output/\",\n",
    "        # which also changes the final destination for our next text file.\n",
    "        currentFolder = \"sample/\"\n",
    "        fileName = re.sub(currentFolder, newDir, fileName)\n",
    "\n",
    "        # Create and open a text file, name it to match its input file,\n",
    "        # and write the OCR'ed text to the file.\n",
    "        with open(fileName + \".txt\", \"w\") as outFile:\n",
    "            outFile.write(ocrText)\n",
    "        \n",
    "        print(fileName, \" successfully created.\")\n",
    "    \n",
    "    # Loop back to check for another image file, run OCR on that file, \n",
    "    # and write its OCR to a new output file. When no more files remain,\n",
    "    # this loop will end, and the script will be finished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the OCR output.\n",
    "\n",
    "In your Finder or File Explorer, locate the [\"sample_output\" folder](sample_output) accompanying this tutorial, and take a look at the text files it should now contain. (Note that we have included these files with the tutorial in case you run into trouble running the script above.) Compare them to the .jpg image files in the \"sample\" folder. What do you notice?\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "Did you look yet?\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "Now that you've looked at all of the files, look more closely at `sessionlawsresol1955nort_0057.jpg` and `sessionlawsresol1955nort_0057.txt`.\n",
    "\n",
    "### Checking for misspellings <a class=\"anchor\" id=\"hyphens\"></a>\n",
    "\n",
    "Although it appears that this page has been entirely correctly OCR'ed, there are two issues that show up in this text file that we want to address in all of our OCR'ed files:\n",
    "\n",
    "1. The original printers **broke words at the end of some lines**. For example, `Dis-trict` and, at the very end of the page, `twenty-`. How do we deal with this without removing words that *should* be hyphenated?\n",
    "\n",
    "\n",
    "2. **How would we know how accurate this simple script might be when applied to the entire volume, or to the entire corpus?** \n",
    "\n",
    "In addition to being hyphenated, `Dis-trict` may be misspelled as `Dis-triet` in our output -- is this just one instance, or does this error recur? If it's recurring, we can use Python to fix it across the corpus. This could be more efficient than having to read the entire OCR'ed corpus. A good starting point is to get a sense of just how accurate the OCR process has been, that is **check its readability**, before we start trying to identify and fix spelling errors.\n",
    "\n",
    "***Note:*** *The errors you see when you run these scripts may vary depending on the version of Tesseract you are using. At the time of writing, these modules rely on Tesseract version 4.1.1. If you see different errors, apply the concepts in this section to different errors and/or documents.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the following script (broken into multiple chunks) we'll check for OCR accuracy by generating a readability score.** During this process, we'll remove the hyphens at the end of lines to help us with spellchecking, but we may find that we introduce new issues for the spellcheck:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To begin, there are a number of modules and libraries we need to import\n",
    "# to extend Python's functionality:\n",
    "\n",
    "# Import PyTesseract and PIL, an image processing library used by PyTesseract, to complete the OCR.\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "# Import os, a module for file management.\n",
    "import os\n",
    "\n",
    "# Import re, a module that we can use to search text.\n",
    "import re\n",
    "\n",
    "# Import glob, a module that helps with file management.\n",
    "import glob\n",
    "\n",
    "# Import the SpellChecker module, which we'll use to look for likely misspelled words.\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Import the word_tokenize module from the nltk (\"Natural Language Processing Kit\") library.\n",
    "# NLTK is a powerful toolset we can use to manipulate and analyze text data.\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# We'll also need the pandas library, which is a powerful toolset for managing data.\n",
    "# We'll learn more about pandas in the exploratory analysis modules.\n",
    "import pandas as pandas\n",
    "\n",
    "# This statement confirms that the above code was run without issue.\n",
    "print(\"Modules & libraries imported. Ready for the next step.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll set up variables that we'll use to give Python \n",
    "# information and structure information that Python returns.\n",
    "# These include the location of the original image files and the\n",
    "# place we want to store our OCR'ed text, as well as a spellcheck\n",
    "# dictionary and a dataframe (essentially, an empty table) we'll use \n",
    "# to structure readability information along with the OCR'ed text.\n",
    "\n",
    "# Open the file folder where our sample pages are stored.\n",
    "# Look only for files ending with the \".jpg\" file extension.\n",
    "sampleFilePath = glob.glob(\"sample/*.jpg\")\n",
    "\n",
    "# Before we loop through each page, we'll augment our spellchecker \n",
    "# dictionary to include place names specific to North Carolina. \n",
    "# Our script for gathering these place names is available here: \n",
    "# https://github.com/UNC-Libraries-data/OnTheBooks/blob/master/examples/adjustment_recommendation/geonames.py\n",
    "\n",
    "# Load the spellchecker dictionary.\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Add the place name words from the \"geonames.txt\" file to the \n",
    "# spellchecker dictionary.\n",
    "spell.word_frequency.load_text_file(\"geonames.txt\")\n",
    "\n",
    "# We'll use Pandas to create a dataframe (an empty table--\n",
    "# explained further in the next tutorial!) that can hold \n",
    "# information about an OCR'ed page and display it in a tabular format.\n",
    "# This dataframe will start out empty with only its column headers \n",
    "# defined. We'll add information to it one page at a time. So each\n",
    "# row will represent 1 page.\n",
    "df = pandas.DataFrame(columns=[\"file_name\",\"token_count\",\"unknown_count\",\"readability\",\"unknown_words\",\"text\"])\n",
    "\n",
    "# This statement confirms that the above code was run without issue.\n",
    "print(\"Variables created. Ready for the next step.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll remove hyphens from the text and run the spellcheck script.\n",
    "\n",
    "# For each file in the sample folder:\n",
    "for file in sampleFilePath:\n",
    "    \n",
    "    # Open a file.\n",
    "    with open(file, 'rb') as inputFile:\n",
    "        \n",
    "        # Get a file name--without the extension-- \n",
    "        # to use when we name the output file.\n",
    "        fileName = os.path.split(file)[1]\n",
    "        \n",
    "        # Read the file using PIL's Image module.\n",
    "        img = Image.open(inputFile)\n",
    "    \n",
    "        # Run OCR on the open file.\n",
    "        ocrText = pytesseract.image_to_string(img)\n",
    "        \n",
    "        # Join hyphenated words that are split between lines by \n",
    "        # looking for a hyphen followed by a newline character: \"-\\n\"\n",
    "            # \"\\n\" is an \"escape character\" and represents the \n",
    "            # \"newline,\" a character that is usually invisible \n",
    "            # to human readers but that computers use to mark the \n",
    "            # end/beginning of a line. Each time you press the \n",
    "            # Enter/Return key on your keyboard, an invisible \"\\n\" \n",
    "            # is created to mark the beginning of a new line.\n",
    "        ocrText = ocrText.replace(\"-\\n\",\"\")\n",
    "        \n",
    "        # Now we'll check spellings and insert corrections!\n",
    "        \n",
    "        # First, we'll use NLTK to \"tokenize\" text. \n",
    "            # \"Tokenize\" here means to take a page of our OCR'ed text,\n",
    "            # which Python is currently reading as one big glob of data,\n",
    "            # and separate each word out so that it can be read as an\n",
    "            # individual piece of data within a larger data structure \n",
    "            # (a list). This process also removes punctuation.\n",
    "        tokens = word_tokenize(ocrText)\n",
    "        \n",
    "        # Next, we'll convert all of those tokens (words) into \n",
    "        # lowercase because the spellcheck dictionary is in all \n",
    "        # lowercase, and the spellcheck process is case sensitive.\n",
    "        tokens = [token for token in tokens if token.isalpha()]\n",
    "        \n",
    "        # We'll make sure that our text data complies with a universal \n",
    "        # text format so that all characters in the data and the \n",
    "        # spellchecker can be matched.\n",
    "        tokens = [token.encode(\"utf-8\", errors = \"replace\") for token in tokens]\n",
    "        \n",
    "        # Now we can get all of the words that don't match the \n",
    "        # spellchecker dictionary or our list of place names--\n",
    "        # these are the potential spelling errors.\n",
    "        unknown = spell.unknown(tokens)\n",
    "        \n",
    "        # Let's use a little math to find out how many potential \n",
    "        # spelling errors were identified. As part of this process, \n",
    "        # we'll create a \"readability\" score that will give us a \n",
    "        # percentage of how readable each file is--how much of the \n",
    "        # OCR'ed is \"correct.\"\n",
    "        \n",
    "        # If the list of unknown tokens (words) is greater than 0 \n",
    "        # (i.e. if the list is not empty):\n",
    "        if len(unknown) != 0:\n",
    "            \n",
    "               # Following order of operations, here's what's happening \n",
    "               # in the readability variable below:\n",
    "               # 1. Divide the number of unknown tokens (len(unknown)) \n",
    "                    # by the total number of tokens on the page\n",
    "                    # (len(tokens)). Use \"float\" to specify that Python\n",
    "                    # returns a decimal number:\n",
    "                        # (float(len(unknown))/float(len(tokens))\n",
    "               # 2. Multiply the number from step 1 by 100.\n",
    "                    # (float(len(unknown))/float(len(tokens)) * 100)\n",
    "               # 3. Subtract the number from step 2 from 100.\n",
    "                    # 100 - (float(len(unknown))/float(len(tokens)) * 100)\n",
    "               # 4. Round the number from step 3 to 2 decimal places\n",
    "                    # round(100 - (float(len(unknown))/float(len(tokens)) * 100), 2)\n",
    "            \n",
    "           readability = round(100 - (float(len(unknown))/float(len(tokens)) * 100), 2)\n",
    "        \n",
    "        # If the list of unknown tokens is empty (or equal to 0), then readability is 100!\n",
    "        else:\n",
    "           readability = 100\n",
    "    \n",
    "        # Let's create a record of the readability information \n",
    "        # for this page that we'll add to the dataframe. \n",
    "        # The following is a Python dictionary, another way of \n",
    "        # storing data. Each word or phrase to the left of the : is a\n",
    "        # \"key\" -- think of it as a column header. Each piece of \n",
    "        # information to the right is a \"value\" -- information \n",
    "        # written in a single cell below each header. \n",
    "        # Altogether, this dictionary represents 1 row (\"imgRecord\") \n",
    "        # in a table (or dataframe).\n",
    "        imgRecord = {\n",
    "                \"file_name\" : fileName,\n",
    "                \"token_count\" : len(tokens),\n",
    "                \"unknown_count\" : len(unknown),\n",
    "                \"readability\" : readability,\n",
    "                \"unknown_words\" : list(unknown),\n",
    "                \"text\" : ocrText\n",
    "                }\n",
    "        \n",
    "        # Here's where we'll add all the information we gathered in \n",
    "        # imgRecord as a row in our dataframe.\n",
    "        df = df.append(imgRecord, ignore_index=True)\n",
    "\n",
    "        \n",
    "        # This statement lets us know if a page has been successfully \n",
    "        # checked for readability.\n",
    "        print(fileName, \"checked for readability.\")\n",
    "    \n",
    "# This time, instead of creating individual .txt files for each page,\n",
    "# we're going to save all of the OCR'ed text and readability \n",
    "# information to a single .csv (\"comma separated value\") file. \n",
    "# We can view this file format as a table. Having everything stored \n",
    "# like this will help us with clean up and future analysis.\n",
    "df.to_csv(r'sample_output/sample_output_spellchecked.csv', header=True, index=True, sep=',')\n",
    "\n",
    "# We have the data stored in a file now, but we can also \n",
    "# preview it here:\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <strong>Take a look at the data preview above.</strong>\n",
    "    <ul>\n",
    "        <li>Can you identify what each of the columns represents? Which columns are you unsure of?</li>\n",
    "        <li>How do you interpret the readability column?</li>\n",
    "        <li>What do you notice about the unknown words column?</li>\n",
    "        <li>What do you notice about the text column?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open [sample_output_spellchecked.csv](sample_output/sample_output_spellchecked.csv) to view the full dataset. You'll find it in the sample_output folder. **Let's take a look at each column:**\n",
    "\n",
    "- **file_name**: The name for the corresponding image file. For now, this is the only information in the table that identifies where the rest of the information in each row comes from (which page).\n",
    "- **token_count**: The total number of tokens (words) found in each page.\n",
    "- **unknown_count**: The number of unknown (\"misspelled\") words found in each page.\n",
    "- **readability**: Think of this as the percentage of the page that was readable.\n",
    "- **unknown_words**: A list of tokens (words or in some cases characters) that were not listed in the spellchecker.\n",
    "- **text**: The OCR'ed text output from each page. The output here includes all <a href=\"https://en.wikipedia.org/wiki/Escape_character#JavaScript\" target=\"blank\">escape characters</a>, so it may look as if a lot of erronenous characters have been added. In the [next tutorial](05-StructuringOCRData.ipynb), we'll see how including these in our OCR'ed text can be useful.\n",
    "\n",
    "**Let's consider what these columns can tell us:**\n",
    "- The number of uknown words in each page is low (3 max.), and the readability score for each is near 100. This means that *on each page there are only a few errors that need to be addressed.*\n",
    "- The list of unknown words shows that there are some errors that repeat on all or most pages. These include `b` and `ch`. A closer look at the text column shows that `b` is part of the escape character formatting that's been added. Meanwhile `ch` is likely the abbreviation for `chapter` that occurs frequently throughout the text. These \"errors\" can be ignored.\n",
    "- What else do the columns tell you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <p><strong>A note on checking spelling:</strong> Python and the spellchecker module use a list of words that <em>we</em> provide to check what is \"correct\" or \"incorrect\" spelling. So the spellcheck process is only as good as the information we provide. If you are working with text that includes abbreviations, non-English words, words written in dialect, or words that are in any other way not \"standard,\" it's important to include them. This may mean taking time to put together a list of words to add to the spellchecker. In some cases, though, this may not be practical--particularly for texts with a lot of dialog written in dialect, or for non-standard spellings. In these cases, you may need to check spellings manually. Here is an example where it is not the <em>algorithm</em> but the <em>data</em> provided <em>to</em> the algorithm that can produce invalid assessments of a text's \"readability.\"</p>\n",
    "    <p><em>What does it mean that these spellcheck tools have been created to prioritize standardized modern language?</em></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's look again at `ch` in the text column. Note that it appears at the beginning of most pages. In some cases, it's written `cu` instead of `ch`, but our spellchecker didn't recognize that as an error. Furthermore, the alternating upper and lowercase characters in `session` were ignored because we changed all words to lowercase for the spellcheck. This should tell us that, although overall readability is high from a computer's perspective, there may be some issues not identified in this process that we need to address in order to make this dataset more *human* readable. We'll look at some ways to better address *human readability* in the [next tutorial](05-StructuringOCRData.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <strong>Review:</strong> \n",
    "    <p>In this module, we covered the basic steps for performing OCR, including how to prepare for OCR, how to run a basic OCR script, and how to assess the readability, or \"accuracy,\" of your OCR output. But this is just the beginning in terms of producing a fully digitized text dataset that we can use for analysis. In the next tutorial, we'll take a look at a few techniques for \"cleaning\" our OCR'ed text in preparation for analysis.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources <a class=\"anchor\" id=\"resources\"></a>\n",
    "\n",
    "### OCR Research & Evaluation\n",
    "\n",
    "- [Improving the quality of the output.](https://tesseract-ocr.github.io/tessdoc/ImproveQuality) *Tesseract documentation.*\n",
    "- Rebecca Bakker. [\"OCR for Digital Collections.\"](https://digitalcommons.fiu.edu/cgi/viewcontent.cgi?article=1047&context=glworks) *FIU Digital Commons.*\n",
    "- Ryan Baumman. [\"Automatic evaluation of OCR quality.\"](https://ryanfb.github.io/etc/2015/03/16/automatic_evaluation_of_ocr_quality.html) */etc.*\n",
    "- Brandon W. Hawk. [\"OCR and Medieval Manuscripts: Establishing a Baseline.\"](https://brandonwhawk.net/2015/04/20/ocr-and-medieval-manuscripts-establishing-a-baseline/) *Brandon W. Hawk.* (This post is a comparison of ABBYY FineReader & Adobe Acrobat OCR technologies as applied to medieval texts.)\n",
    "- Ray Smith. [\"An Overview of the Tesseract OCR Engine.\"](https://research.google/pubs/pub33418/) *Google Research.*\n",
    "- Simon Tanner. [\"Deciding whether Optical Character Recognition is feasible.\"](https://www.kb.nl/sites/default/files/docs/OCRFeasibility_final.pdf) *King's Digital Consultancy Services.*\n",
    "\n",
    "### Tutorials\n",
    "\n",
    "*The following is a list of tutorials that include different scholars' approaches to OCR. Some also use Tesseract, but most use different scripting or programming languages. There is no single best way to do OCR, so if you have the time they worth trying to see which works best for your project.*\n",
    "\n",
    "- Andrew Akhlaghi. [\"OCR and Machine Translation.\"](http://programminghistorian.org/en/lessons/OCR-and-Machine-Translation) *The Programming Historian.* (Note that this tutorial uses Tesseract but works with the bash scripting language instead of Python.)\n",
    "- Ryan Baumman. [\"Command-Line OCR with Tesseract on Mac OS X.\"](https://ryanfb.github.io/etc/2014/11/13/command_line_ocr_on_mac_os_x.html) */etc.*\n",
    "- Shawn Graham. \"Extracting Text from PDFs; Doing OCR; all within R.\" *Electric Archaeology.* (This blog post describes a method for OCR using the R programming language.)\n",
    "- Moritz Mhr. [\"Working with batches of PDF files.\"](https://programminghistorian.org/en/lessons/working-with-batches-of-pdf-files) *The Programming Historian.* (Note that this tutorial uses Tesseract and works in the command line without Python.)\n",
    "- Rebecca Tarnopol. [\"How to OCR Documents for Free in Google Drive.\"](https://business.tutsplus.com/tutorials/how-to-ocr-documents-for-free-in-google-drive--cms-20460) *TutsPlus.*\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "*In case you are interested in scholars' and practioners' debates around OCR.\n",
    "\n",
    "- Karen Coyle. [\"Digital Urtext.\"](https://kcoyle.blogspot.com/2012/04/digital-urtext.html) *Coyle's InFormation.*\n",
    "- *Humanities Commons* [search for \"OCR\"](https://hcommons.org/?s=ocr).\n",
    "- Ray Smith, Daria Antonova, and Dar-Shyang Lee. [\"Adapting the Tesseract open source OCR engine for multilingual OCR.\"](https://dl.acm.org/doi/10.1145/1577802.1577804) MOCR '09: Proceedings of the International Workshop on Multilingual OCR.\n",
    "- Ted Underwood. [\"The challenges of digital work on early-19c collections.\"](https://tedunderwood.com/2011/10/07/the-challenges-of-digital-work-on-early-19c-collections/) *The Stone and the Shell.*\n",
    "- [TranScriptorium's handwritten text recognition project results.](https://cordis.europa.eu/project/id/600707/results)\n",
    "- [\"How to Transcribe Documents with Transkribus - Introduction.\"](https://readcoop.eu/transkribus/howto/how-to-transcribe-documents-with-transkribus-introduction/) *Read Coop.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**>> Next module: [Structuring OCR'ed Text as Data](05-StructuringOCRData.ipynb) >>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This module is licensed under the [GNU General Public License v3.0](https://github.com/UNC-Libraries-data/OnTheBooks/blob/master/LICENSE). Individual images and data files associated with this module may be subject to a different license. If so, we indicate this in the module text.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
